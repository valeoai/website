<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://valeoai.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://valeoai.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-10T11:26:57+00:00</updated><id>https://valeoai.github.io//feed.xml</id><title type="html">valeo.ai</title><subtitle>valeo.ai research page </subtitle><entry><title type="html">valeo.ai at ECCV 2024</title><link href="https://valeoai.github.io//posts/2024-09-25-valeoai-at-eccv-2024/" rel="alternate" type="text/html" title="valeo.ai at ECCV 2024"/><published>2024-09-25T00:00:00+00:00</published><updated>2024-09-25T00:00:00+00:00</updated><id>https://valeoai.github.io//posts/valeoai-at-eccv-2024</id><content type="html" xml:base="https://valeoai.github.io//posts/2024-09-25-valeoai-at-eccv-2024/"><![CDATA[<p>The <a href="https://eccv.ecva.net/">European Conference on Computer Vision (ECCV)</a> is a biennial landmark conference for the increasingly large community of researchers in computer vision and machine learning from both academia and industry. At the 2024 edition the valeo.ai team will present five papers in the main conference and four in the workshops. We are also organizing two tutorials (<a href="https://uqtutorial.github.io/">Bayesian Odyssey</a> and <a href="https://shashankvkt.github.io/eccv2024-SSLBIG-tutorial.github.io/">Time is precious: Self-Supervised Learning Beyond Images</a>), the <a href="https://uncertainty-cv.github.io/2024/">Uncertainty Quantification for Computer Vision</a> workshop, a talk at the <a href="https://sites.google.com/view/omnilabel-workshop-eccv24/program">OmniLabel workshop</a>, and the <a href="https://github.com/valeoai/bravo_challenge">BRAVO challenge</a>. Our team has also contributed to the reviewing process, with seven reviewers, three area chairs, and two outstanding reviewer awards. The team will be at ECCV to present these works and will be happy to discuss more about these projects and ideas, and share our exciting ongoing research.</p> <p><img src="/assets/img/posts/2024_eccv/valeoai_eccv.jfif" alt="valeo.ai team at ECCV 2024" height="100%" width="100%"/></p> <h2 id="train-till-you-drop-towards-stable-and-robust-source-free-unsupervised-3d-domain-adaptation">Train Till You Drop: Towards Stable and Robust Source-free Unsupervised 3D Domain Adaptation</h2> <h3 id="authors-bjoern-michele--alexandre-boulch--tuan-hung-vu--gilles-puy--renaud-marlet--nicolas-courty">Authors: <a href="https://bjoernmichele.com">Bjoern Michele</a>    <a href="https://boulch.eu/">Alexandre Boulch</a>    <a href="https://tuanhungvu.github.io/">Tuan-Hung Vu</a>    <a href="https://sites.google.com/site/puygilles/">Gilles Puy</a>    <a href="http://imagine.enpc.fr/~marletr/">Renaud Marlet</a>    <a href="https://people.irisa.fr/Nicolas.Courty/">Nicolas Courty</a></h3> <h4 align="center"> [<a href="https://arxiv.org/abs/2409.04409">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/TTYD">Code</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/publications/ttyd/">Project page</a>]</h4> <p><img src="/assets/img/publications/2024_ttyd/featured.png" alt="ttdy_overview" height="100%" width="100%"/></p> <div class="caption"><b>Evolution of the performance of baselines without degradation prevention strategies as they train over 20k iterations.</b> Our method (TTYDcore) uses an unsupervised criterion to stop training. The horizontal dotted line illustrates that we keep the model obtained at the stopping point (marked with a cross). </div> <p>We tackle the challenging problem of source-free unsupervised domain adaptation (SFUDA) for 3D semantic segmentation. It amounts to performing domain adaptation on an unlabeled target domain without any access to source data; the available information is a model trained to achieve good performance on the source domain. A common issue with existing SFUDA approaches is that performance degrades after some training time, which is a by-product of an under-constrained and ill-posed problem. We discuss two strategies to alleviate this issue. First, we propose a sensible way to regularize the learning problem. Second, we introduce a novel criterion based on agreement with a reference model. It is used (1) to stop the training when appropriate and (2) as validator to select hyperparameters without any knowledge on the target domain. Our contributions are easy to implement and readily amenable for all SFUDA methods, ensuring stable improvements over all baselines. We validate our findings on various 3D lidar settings, achieving state-of-the-art performance.</p> <p><img src="/assets/img/posts/2024_eccv/ttyd_results.PNG" alt="ttyd_results" height="100%" width="100%"/></p> <div class="caption"><b>Examples of results with TTYDstop</b>: ground truth (GT), initial model trained only on source data, training with our training scheme when using our stopping criterion, and “full” training for 20k iterations. Notable errors due to degradation are marked with a dashed rectangle. </div> <hr/> <h2 id="unitraj-a-unified-framework-for-scalable-vehicle-trajectory-prediction">UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction</h2> <h4 id="authors-lan-feng--mohammadhossein-bahari--kaouther-messaoud-ben-amor--éloi-zablocki--matthieu-cord--alexandre-alahi">Authors: <a href="https://alan-lanfeng.github.io/">Lan Feng</a>    <a href="https://mohammadhossein-bahari.github.io/">Mohammadhossein Bahari</a>    <a href="https://scholar.google.com/citations?user=X0teZIAAAAAJ">Kaouther Messaoud Ben Amor</a>    <a href="https://scholar.google.fr/citations?user=dOkbUmEAAAAJ">Éloi Zablocki</a>    <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a>    <a href="https://people.epfl.ch/alexandre.alahi">Alexandre Alahi</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2403.15098">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/vita-epfl/unitraj">Code</a>] &nbsp;&nbsp; [<a href="https://vita-epfl.github.io/UniTraj/">Project page</a>] &nbsp;&nbsp; [<a href="https://www.youtube.com/watch?v=2IzuUtiNA_4">Video</a>]</h4> <p>Vehicle trajectory prediction has increasingly relied on data-driven solutions, but their ability to scale to different data domains and the impact of larger dataset sizes on their generalization remain under-explored. While these questions can be studied by employing multiple datasets, it is challenging due to several discrepancies, e.g., in data formats, map resolution, and semantic annotation types. To address these challenges, we introduce UniTraj, a comprehensive framework that unifies various datasets, models, and evaluation criteria, presenting new opportunities for the vehicle trajectory prediction field. In particular, using UniTraj, we conduct extensive experiments and find that model performance significantly drops when transferred to other datasets. However, enlarging data size and diversity can substantially improve performance, leading to a new state-of-the-art result for the nuScenes dataset. We provide insights into dataset characteristics to explain these findings.</p> <p><img src="/assets/img/publications/2024_unitraj/unitraj.PNG" alt="unitraj_overview" height="100%" width="100%"/></p> <div class="caption"><b>Overview of UniTraj: a unified platform for comprehensive research in trajectory prediction.</b> </div> <hr/> <h2 id="lost-and-found-overcoming-detector-failures-in-online-multi-object-tracking">Lost and Found: Overcoming Detector Failures in Online Multi-Object Tracking</h2> <h4 id="authors-lorenzo-vaquero--yihong-xu--xavier-alameda-pineda--víctor-m-brea--manuel-mucientes">Authors: <a href="https://citius.gal/team/lorenzo-vaquero-otal/">Lorenzo Vaquero</a>    <a href="https://github.com/yihongXU">Yihong Xu</a>    <a href="https://xavirema.eu/">Xavier Alameda-Pineda</a>    <a href="https://citius.gal/team/victor-manuel-brea-sanchez/">Víctor M. Brea</a>    <a href="https://persoal.citius.usc.es/manuel.mucientes/">Manuel Mucientes</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2407.10151">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/lorenzovaquero/BUSCA">Code</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/publications/busca/">Project page</a>]</h4> <p>Multi-object tracking (MOT) endeavors to precisely estimate the positions and identities of multiple objects over time. The prevailing approach, tracking-by-detection (TbD), first detects objects and then links detections, resulting in a simple yet effective method. However, contemporary detectors may occasionally miss some objects in certain frames, causing trackers to cease tracking prematurely. To tackle this issue, we propose BUSCA, meaning ‘to search’, a versatile framework compatible with any online TbD system, enhancing its ability to persistently track those objects missed by the detector, primarily due to occlusions. Remarkably, this is accomplished without modifying past tracking results or accessing future frames, i.e., in a fully online manner. BUSCA generates proposals based on neighboring tracks, motion, and learned tokens. Utilizing a decision Transformer that integrates multimodal visual and spatiotemporal information, it addresses the object-proposal association as a multi-choice question-answering task. BUSCA is trained independently of the underlying tracker, solely on synthetic data, without requiring fine-tuning. Through BUSCA, we showcase consistent performance enhancements across five different trackers and establish a new state-of-the-art baseline across three different benchmarks.</p> <p><img src="/assets/img/publications/2024_busca/busca.png" alt="busca_overview" height="90%" width="90%"/></p> <div class="caption"><b>Overview of BUSCA</b>: Enhancing multi-object trackers by finding undetected objects. </div> <hr/> <h2 id="clip-dinoiser-teaching-clip-a-few-dino-tricks-for-open-vocabulary-semantic-segmentation">CLIP-DINOiser: Teaching CLIP a few DINO tricks for open-vocabulary semantic segmentation</h2> <h4 id="authors-monika-wysoczańska-oriane-siméoni--michaël-ramamonjisoa-andrei-bursuc-tomasz-trzciński--patrick-pérez">Authors: <a href="https://wysoczanska.github.io/">Monika Wysoczańska</a>  <a href="https://osimeoni.github.io/">Oriane Siméoni</a>   <a href="https://michaelramamonjisoa.github.io/">Michaël Ramamonjisoa</a>  <a href="https://abursuc.github.io/">Andrei Bursuc</a>  <a href="https://scholar.google.com/citations?hl=en&amp;user=bJMRBFoAAAAJ">Tomasz Trzciński</a>   <a href="https://ptrckprz.github.io/">Patrick Pérez</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2312.12359">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/wysoczanska/clip_dinoiser/">Code</a>] &nbsp;&nbsp; [<a href="https://wysoczanska.github.io/CLIP_DINOiser/">Project page</a>]</h4> <p>The popular CLIP model displays impressive zero-shot capabilities thanks to its seamless interaction with arbitrary text prompts. However, its lack of spatial awareness makes it unsuitable for dense computer vision tasks, e.g., semantic segmentation, without an additional fine-tuning step that often uses annotations and can potentially suppress its original open-vocabulary properties. Meanwhile, self-supervised representation methods have demonstrated good localization properties without human-made annotations nor explicit supervision. In this work, we take the best of both worlds and propose an open-vocabulary semantic segmentation method, which does not require any annotations.</p> <p><img src="/assets/img/publications/2024_dinoiser/dinoiser-examples.png" alt="dinoiser_example" height="100%" width="100%"/></p> <div class="caption"><b>Examples of open-vocabulary semantic segmentation results obtained with our method CLIP-DINOiser on ‘in-the-wild’ images vs. those of MaskCLIP.</b> </div> <p>We propose to locally improve dense MaskCLIP features, which are computed with a simple modification of CLIP’s last pooling layer, by integrating localization priors extracted from self-supervised features from DINO. By doing so, we greatly improve the performance of MaskCLIP and produce smooth outputs. Moreover, we show that the used self-supervised feature properties can directly be learnt from CLIP features. Our method CLIP-DINOiser needs only a single forward pass of CLIP and two light convolutional layers at inference, no extra supervision nor extra memory and reaches state-of-the-art results on challenging and fine-grained benchmarks such as COCO, Pascal Context, Cityscapes and ADE20k.</p> <p><img src="/assets/img/posts/2024_eccv/dinoiser_overview.PNG" alt="dinoiser_overview" height="100%" width="100%"/></p> <div class="caption"><b>Overview of CLIP-DINOiser.</b> We use DINO as a teacher which ‘teaches’ CLIP how to extract localization information with similar patch correlations. At inference, an input image is forwarded through the frozen CLIP image backbone and MaskCLIP projection. The produced features are then improved with our pooling strategy which is guided by correlations predicted with a trained convolutional layer applied on CLIP.</div> <hr/> <h2 id="reliability-in-semantic-segmentation-can-we-use-synthetic-data">Reliability in Semantic Segmentation: Can We Use Synthetic Data</h2> <h4 id="authors-thibaut-loiseau---tuan-hung-vu---mickael-chen--patrick-pérez--matthieu-cord">Authors: <a href="https://imagine-lab.enpc.fr/staff-members/thibaut-loiseau/">Thibaut Loiseau</a>    <a href="https://tuanhungvu.github.io/">Tuan-Hung Vu</a>    <a href="https://scholar.google.fr/citations?user=QnRpMJAAAAAJ">Mickael Chen</a>    <a href="https://ptrckprz.github.io/">Patrick Pérez</a>    <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2312.09231">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/GenVal">Code</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/publications/genval">Project page</a>]</h4> <p>Assessing the reliability of perception models to covariate shifts and out-of-distribution (OOD) detection is crucial for safety-critical applications such as autonomous vehicles. By nature of the task, however, the relevant data is difficult to collect and annotate. In this paper, we challenge cutting-edge generative models to automatically synthesize data for assessing reliability in semantic segmentation. By fine-tuning Stable Diffusion, we perform zero-shot generation of synthetic data in OOD domains or inpainted with OOD objects. Synthetic data is employed to provide an initial assessment of pretrained segmenters, thereby offering insights into their performance when confronted with real edge cases. Through extensive experiments, we demonstrate a high correlation between the performance on synthetic data and the performance on real OOD data, showing the validity approach. Furthermore, we illustrate how synthetic data can be utilized to enhance the calibration and OOD detection capabilities of segmenters.</p> <p><img src="/assets/img/publications/2024_genval/genval-overview.PNG" alt="genval_overview" height="100%" width="100%"/></p> <div class="caption"><b>Assessing 40 pretrained segmenters under covariate shifts.</b> Segmentation models under scrutiny were trained on Cityscapes train set only (in-domain data). They are evaluated on (i) Cityscapes validation set, (ii) real OOD data, and (iii) proposed synthetic data. We observe a strong correlation between results on (ii) and (iii). </div> <hr/> <h2 id="valeo4cast-a-modular-approach-to-end-to-end-forecasting">Valeo4Cast: A Modular Approach to End-to-End Forecasting</h2> <p class="page-description"><a href="https://sites.google.com/view/road-eccv2024/home">ECCV 2024 ROAD++ Workshop</a></p> <p class="page-description"><a href="https://www.argoverse.org/E2EForecasting.html">Winning solution in Argoverse 2 Unified Detection, Tracking and Forecasting Challenge</a></p> <h4 id="authors-yihong-xu-éloi-zablocki--alexandre-boulch-gilles-puy---mickaël-chen-florent-bartoccioni--nermin-samet---oriane-siméoni---spyros-gidaris---tuan-hung-vu-andrei-bursuc--eduardo-valle-renaud-marlet--matthieu-cord">Authors: <a href="https://scholar.google.fr/citations?user=vMLRRVkAAAAJ">Yihong Xu</a>, <a href="https://scholar.google.fr/citations?user=dOkbUmEAAAAJ">Éloi Zablocki</a>, <a href="https://www.boulch.eu/">Alexandre Boulch</a>, <a href="https://sites.google.com/site/puygilles/home">Gilles Puy</a>, <a href="https://scholar.google.com/citations?user=QnRpMJAAAAAJ">Mickaël Chen</a>, <a href="https://f-barto.github.io/">Florent Bartoccioni</a>, <a href="https://nerminsamet.github.io/">Nermin Samet</a>, <a href="https://osimeoni.github.io/">Oriane Siméoni</a>, <a href="https://scholar.google.fr/citations?user=7atfg7EAAAAJ">Spyros Gidaris</a>, <a href="https://tuanhungvu.github.io/">Tuan-Hung Vu</a>, <a href="https://abursuc.github.io/">Andrei Bursuc</a>, <a href="https://scholar.google.com/citations?user=lxWPqWAAAAAJ">Eduardo Valle</a>, <a href="http://imagine.enpc.fr/~marletr/">Renaud Marlet</a>, <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2406.08113">Paper</a>] &nbsp;&nbsp; [<a href="https://eval.ai/web/challenges/challenge-page/2006/leaderboard/4752">leaderboard</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/publications/valeo4cast/">page</a>]</h4> <p>Motion forecasting is crucial in autonomous driving systems to anticipate the future trajectories of surrounding agents such as pedestrians, vehicles, and traffic signals. In end-to-end forecasting, the model must jointly detect from sensor data (cameras or LiDARs) the position and past trajectories of the different elements of the scene and predict their future location. We depart from the current trend of tackling this task via end-to-end training from perception to forecasting and we use a modular approach instead. Following a recent study, we individually build and train detection, tracking, and forecasting modules. We then only use consecutive finetuning steps to integrate the modules better and alleviate compounding errors. Our study reveals that this simple yet effective approach significantly improves performance on the end-to-end forecasting benchmark. Consequently, our solution ranks first in the Argoverse 2 end-to-end Forecasting Challenge held at CVPR 2024 Workshop on Autonomous Driving (WAD), with 63.82 mAPf. We surpass forecasting results by +17.1 points over last year’s winner and by +13.3 points over this year’s runner-up. This remarkable performance in forecasting can be explained by our modular paradigm, which integrates finetuning strategies and significantly outperforms the end-to-end-trained counterparts.</p> <p><img src="/assets/img/publications/2024_valeo4cast/valeo4cast.PNG" alt="valeo4cast_overview" height="80%" width="80%"/></p> <div class="caption"><b>Valeo4Cast overview.</b> </div> <hr/> <h2 id="pafuse-part-based-diffusion-for-3d-whole-body-pose-estimation">PAFUSE: Part-based Diffusion for 3D Whole-Body Pose Estimation</h2> <p class="page-description"><a href="https://sites.google.com/view/t-cap-2024/home">ECCV 2024 Workshop Towards a Complete Analysis of People (T-CAP)</a></p> <h4 id="authors-nermin-samet--cédric-rommel--david-picard--eduardo-valle">Authors: <a href="https://nerminsamet.github.io/">Nermin Samet</a>    <a href="https://cedricrommel.github.io/">Cédric Rommel</a>    <a href="https://davidpicard.github.io/">David Picard</a>    <a href="https://eduardovalle.com/">Eduardo Valle</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2407.10220">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/PAFUSE">Code</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/publications/pafuse">Project page</a>]</h4> <p>We introduce a novel approach for 3D whole-body pose estimation, addressing the challenge of scale- and deformability- variance across body parts brought by the challenge of extending the 17 major joints on the human body to fine-grained keypoints on the face and hands. In addition to addressing the challenge of exploiting motion in unevenly sampled data, we combine stable diffusion to a hierarchical part representation which predicts the relative locations of fine-grained keypoints within each part (e.g., face) with respect to the part’s local reference frame. On the H3WB dataset, our method greatly outperforms the current state of the art, which fails to exploit the temporal information. We also show considerable improvements compared to other spatiotemporal 3D human-pose estimation approaches that fail to account for the body part specificities.</p> <p><img src="/assets/img/posts/2024_eccv/pafuse.PNG" alt="pafuse_overview" height="100%" width="100%"/></p> <div class="caption"><b>Overview of PAFUSE.</b> </div> <hr/> <h2 id="llm-wrapper-black-box-semantic-aware-adaptation-of-vision-language-foundation-models">LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Foundation Models</h2> <p class="page-description"><a href="https://sites.google.com/view/eval-fomo-24/home">ECCV 2024 Workshop on Emergent Visual Abilities and Limits of Foundation Models (Eval-FoMo)</a></p> <h4 id="authors-amaia-cardiel--éloi-zablocki--oriane-siméoni--elias-ramzi--matthieu-cord">Authors: Amaia Cardiel    <a href="https://scholar.google.fr/citations?user=dOkbUmEAAAAJ">Éloi Zablocki</a>    <a href="https://osimeoni.github.io/">Oriane Siméoni</a>    <a href="https://elias-ramzi.github.io/">Elias Ramzi</a>    <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2409.11919">Paper</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/publications/llm_wrapper">Project page</a>]</h4> <p>Vision Language Models (VLMs) have shown impressive performances on numerous tasks but their zero-shot capabilities can be limited compared to dedicated or fine-tuned models. Yet, fine-tuning VLMs comes with strong limitations as it requires a ‘white-box’ access to the model’s architecture and weights while some recent models are proprietary (e.g., Grounding DINO 1.5). It also requires expertise to design the fine-tuning objectives and optimize the hyper-parameters, which are specific to each VLM and downstream task. In this work, we propose LLM-wrapper, a novel approach to adapt VLMs in a ‘black-box’ and semantic-aware manner by leveraging large language models (LLMs) so as to reason on their outputs.</p> <p><img src="/assets/img/publications/2024_llm_wrapper/llm_wrapper.PNG" alt="llm-wrapper_overview" height="80%" width="80%"/></p> <div class="caption"><b>Overview of LLM-Wrapper.</b> </div> <p>We demonstrate the effectiveness of LLM-wrapper on Referring Expression Comprehension (REC), a challenging open-vocabulary task that requires spatial and semantic reasoning. Our approach significantly boosts the performance of off-the-shelf models, yielding results that are on par or competitive when compared with classic VLM fine-tuning (cf ‘FT VLM’ in our main results). Despite a few failure cases due to the LLM ‘blindness’ (cf Qualitative results, bottom right)), LLM-wrapper shows better semantic, spatial and relational reasoning, as illustrated in our qualitative results below.</p> <p><img src="/assets/img/posts/2024_eccv/llm_wrapper_results.PNG" alt="llm-wrapper_results" height="90%" width="90%"/></p> <div class="caption"><b>LLM-Wrapper results.</b> </div> <hr/> <h2 id="regents-real-world-safety-critical-driving-scenario-generation-made-stable">ReGentS: Real-World Safety-Critical Driving Scenario Generation Made Stable</h2> <p class="page-description"><a href="https://coda-dataset.github.io/w-coda2024/">ECCV 2024 Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving (W-CODA)</a></p> <h4 id="authors-yuan-yin--pegah-khayatan--éloi-zablocki--alexandre-boulch--matthieu-cord">Authors: <a href="https://yuan-yin.github.io/">Yuan Yin</a>    <a href="https://pegah-kh.github.io/">Pegah Khayatan</a>    <a href="https://scholar.google.fr/citations?user=dOkbUmEAAAAJ">Éloi Zablocki</a>    <a href="https://www.boulch.eu/">Alexandre Boulch</a>    <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2409.07830">Paper</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/publications/regents">Project page</a>]</h4> <p>Machine learning based autonomous driving systems often face challenges with safety-critical scenarios that are rare in real-world data, hindering their large-scale deployment. While increasing real-world training data coverage could address this issue, it is costly and dangerous. This work explores generating safety-critical driving scenarios by modifying complex real-world regular scenarios through trajectory optimization. We propose ReGentS, which stabilizes generated trajectories and introduces heuristics to avoid obvious collisions and optimization problems. Our approach addresses unrealistic diverging trajectories and unavoidable collision scenarios that are not useful for training robust planner. We also extend the scenario generation framework to handle real-world data with up to 32 agents. Additionally, by using a differentiable simulator, our approach simplifies gradient descent-based optimization involving a simulator, paving the way for future advancements.</p> <p><img src="/assets/img/publications/2024_regents/regents_page.png" alt="regents_overview" height="100%" width="100%"/></p> <hr/> <h2 id="the-bravo-semantic-segmentation-challenge-results-in-uncv2024">The BRAVO Semantic Segmentation Challenge Results in UNCV2024</h2> <p class="page-description"><a href="https://uncertainty-cv.github.io/2024/challenge/">ECCV 2024 Workshop on Uncertainty Quantification for Computer Vision</a></p> <h4 id="authors-tuan-hung-vu---eduardo-valle--andrei-bursuc--tommie-kerssies--daan-de-geus--gijs-dubbelman--long-qian--bingke-zhu--yingying-chen--ming-tang--jinqiao-wang--tomáš-vojíř--jan-šochman--jiří-matas--michael-smith--frank-ferrie--shamik-basu--christos-sakaridis--luc-van-gool">Authors: <a href="https://tuanhungvu.github.io/">Tuan-Hung Vu</a>    <a href="https://eduardovalle.com/">Eduardo Valle</a>    <a href="https://abursuc.github.io/">Andrei Bursuc</a>    <a href="">Tommie Kerssies</a>    <a href="">Daan de Geus</a>    <a href="">Gijs Dubbelman</a>    <a href="">Long Qian</a>    <a href="">Bingke Zhu</a>    <a href="">Yingying Chen</a>    <a href="">Ming Tang</a>    <a href="">Jinqiao Wang</a>    <a href="">Tomáš Vojíř</a>    <a href="">Jan Šochman</a>    <a href="">Jiří Matas</a>    <a href="">Michael Smith</a>    <a href="">Frank Ferrie</a>    <a href="">Shamik Basu</a>    <a href="">Christos Sakaridis</a>    <a href="">Luc Van Gool</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2409.15107">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/bravo_challenge">Code</a>] &nbsp;&nbsp; [<a href="https://benchmarks.elsa-ai.eu/?ch=1&amp;com=introduction">Project page</a>]</h4> <p>We propose the unified BRAVO challenge to benchmark the reliability of semantic segmentation models under realistic perturbations and unknown out-of-distribution (OOD) scenarios. We define two categories of reliability: (1) semantic reliability, which reflects the model’s accuracy and calibration when exposed to various perturbations; and (2) OOD reliability, which measures the model’s ability to detect object classes that are unknown during training. The challenge attracted nearly 100 submissions from international teams representing notable research institutions. The results reveal interesting insights into the importance of large-scale pre-training and minimal architectural design in developing robust and reliable semantic segmentation models.</p> <p><img src="/assets/img/publications/2024_bravo/bravo-overview.PNG" alt="bravo_overview" height="80%" width="80%"/></p> <div class="caption"><b>All submissions.</b> Aggregated metrics (out-of-distribution and semantic) on axes, ranking metric (BRAVO Index) on level set. More freedom on the training dataset (Task 2, in orange) did not translate into better results. </div>]]></content><author><name></name></author><category term="3d perception"/><category term="multi-sensor"/><category term="limited supervision"/><category term="reliability"/><category term="motion forecasting"/><category term="robustness"/><category term="generalization"/><category term="driving"/><summary type="html"><![CDATA[Björn Michele, Amaia Cardiel, Yuan Yin, Yihong Xu, Nermin Samet, Tuan-Hung Vu, Andrei Bursuc, Éloi Zablocki]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io//assets/img/posts/2024_eccv/eccv_banner.png"/><media:content medium="image" url="https://valeoai.github.io//assets/img/posts/2024_eccv/eccv_banner.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">valeo.ai at CVPR 2024</title><link href="https://valeoai.github.io//posts/2024-06-13-valeoai-at-cvpr-2024/" rel="alternate" type="text/html" title="valeo.ai at CVPR 2024"/><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://valeoai.github.io//posts/valeoai-at-cvpr-2024</id><content type="html" xml:base="https://valeoai.github.io//posts/2024-06-13-valeoai-at-cvpr-2024/"><![CDATA[<p>The <a href="https://cvpr.thecvf.com/">IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)</a> is a key event for researchers and engineers working on computer vision and machine learning. At the 2024 edition the <a href="https://ptrckprz.github.io/valeoai/">valeo.ai</a> team will present eight <a href="https://valeoai.github.io/publications/">papers</a> in the main conference, two <a href="https://valeoai.github.io/publications/">papers</a> in workshops, and one workshop <a href="https://opendrivelab.com/cvpr2024/workshop/">keynote</a>. Also, the team will present its <a href="https://valeoai.github.io/publications/valeo4cast/">winning solution</a> to the Argoverse 2 <a href="https://www.argoverse.org/E2EForecasting.html">“Unified Detection, Tracking and Forecasting”</a> challenge held at the Workshop on Autonomous Driving. The team will be at CVPR to present these works and will be happy to discuss more about these projects and ideas, and share our exciting ongoing research. We outline our team papers below.</p> <p><img src="/assets/img/posts/2024_cvpr/valeoai_cvpr.jfif" alt="valeo.ai team at CVPR 2024" height="100%" width="100%"/></p> <h2 id="three-pillars-improving-vision-foundation-model-distillation-for-lidar">Three Pillars Improving Vision Foundation Model Distillation for Lidar</h2> <h4 id="authors-gilles-puy-spyros-gidaris-alexandre-boulch-oriane-siméoni--corentin-sautier--andrei-bursuc--patrick-pérez-renaud-marlet">Authors: <a href="https://sites.google.com/site/puygilles/home">Gilles Puy</a>, <a href="https://scholar.google.fr/citations?user=7atfg7EAAAAJ&amp;hl=en">Spyros Gidaris</a>, <a href="https://www.boulch.eu/">Alexandre Boulch</a>, <a href="https://osimeoni.github.io/">Oriane Siméoni</a>, <a href="https://csautier.github.io/">Corentin Sautier</a>, <a href="https://abursuc.github.io/">Andrei Bursuc</a>, <a href="https://ptrckprz.github.io/">Patrick Pérez</a>, <a href="http://imagine.enpc.fr/~marletr/">Renaud Marlet</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2310.17504">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/ScalR">Code</a>] &nbsp;&nbsp; [<a href="https://youtu.be/yksj5WuJY4I">Video</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/publications/scalr/">Project page</a>]</h4> <p>Self-supervised image backbones can be used to address complex 2D tasks (e.g., semantic segmentation, object discovery) very efficiently and with little or no downstream supervision. Ideally, 3D backbones for lidar should be able to inherit these properties after distillation of these powerful 2D features. The most recent methods for image-to-lidar distillation on autonomous driving data show promising results, obtained thanks to distillation methods that keep improving. Yet, we still notice a large performance gap when measuring by linear probing the quality of distilled vs fully supervised features.</p> <p>In this work, instead of focusing only on the distillation method, we study the effect of three pillars for distillation: the 3D backbone, the pretrained 2D backbone, and the pretraining 2D+3D dataset. In particular, thanks to our scalable distillation method named ScaLR, we show that scaling the 2D and 3D backbones and pretraining on diverse datasets leads to a substantial improvement of the feature quality. This allows us to significantly reduce the gap between the quality of distilled and fully-supervised 3D features, and to improve the robustness of the pretrained backbones to domain gaps and perturbations. We show that scaling the 2D and 3D backbones, and pretraining on diverse datasets leads to considerable improvements of the feature quality. The role of these pillars is actually more important than the distillation method itself, which we simplify for easier scaling.</p> <p><img src="/assets/img/posts/2024_cvpr/scalr_overview.PNG" alt="scalr_overview" height="100%" width="100%"/></p> <div class="caption"><b>ScaLR image-to-lidar distillation method with the three pillars studied in this work.</b> </div> <p>In this work, after proposing and studying a scalable distillation method, which we call ScaLR for Scalable Lidar Representation (see Figure above), we make the following contributions.</p> <p>First, we are able to significantly reduce the gap between distilled and supervised lidar representations: on nuScenes, we increase the performance by 22.8 mIoU percentage points compared to the former best distillation method.</p> <p>Second, we show it is possible to pretrain a single backbone on a mixture of datasets, performing similarly or better than separate backbones specialized on each dataset individually. The capacity of this backbone in providing good features across multiple datasets is illustrated in the figure below. For each scene in this figure, we pick a point located on a car and present the feature correlation map with respect to this point. We notice that the most correlated points also belong to cars on all datasets, illustrating the capacity of our single pretrained backbone to correctly distinguish objects on multiple datasets.</p> <p><img src="/assets/img/posts/2024_cvpr/scalr_results.PNG" alt="scalr_results" height="100%" width="100%"/></p> <div class="caption"><b>Correlation maps with a point located on a car</b> on four different scenes extracted from nuScenes, SemanticKITTI, PandaSet-64 and PandaSet-GT, respectively. The features used to compute these maps are extracted from a single pretrained backbone on all four datasets with ScaLR. Color goes from blue to red for low and high values. </div> <p>Third, we thoroughly study the properties of our distilled features. We show that they are robust to both domain gaps and perturbations. We also show that pretraining on diverse datasets improves robustness.</p> <p>Finally, we show that a possible way to get even better features is to distill the knowledge from multiple vision foundation models at the same time, which can be easily done with our scalable distillation strategy</p> <hr/> <h2 id="pointbev-a-sparse-approach-to-bev-predictions">PointBeV: A Sparse Approach to BeV Predictions</h2> <h4 id="authors-loïck-chambon-éloi-zablocki-mickaël-chen-florent-bartoccioni--patrick-pérez---matthieu-cord">Authors: <a href="https://loickch.github.io/">Loïck Chambon</a>, <a href="https://scholar.google.fr/citations?user=dOkbUmEAAAAJ">Éloi Zablocki</a>, <a href="https://scholar.google.com/citations?user=QnRpMJAAAAAJ">Mickaël Chen</a>, <a href="https://f-barto.github.io/">Florent Bartoccioni</a>, <a href="https://ptrckprz.github.io/">Patrick Pérez</a>, <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2312.00703">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/pointbev">Code</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/publications/pointbev/">Project page</a>]</h4> <p>Bird’s-eye View (BeV) representations have emerged as the de-facto shared space in driving applications, offering a unified space for sensor data fusion and supporting various downstream tasks. However, conventional models use grids with fixed resolution and range and face computational inefficiencies due to the uniform allocation of resources across all cells. To address this, we propose PointBeV, a novel sparse BeV segmentation model operating on sparse BeV cells instead of dense grids. This approach offers precise control over memory usage, enabling the use of long temporal contexts and accommodating memory-constrained platforms. PointBeV employs an efficient two-pass strategy for training, enabling focused computation on regions of interest. At inference time, it can be used with various memory/performance trade-offs and flexibly adjusts to new specific use cases.</p> <p><img src="/assets/img/publications/2024_pointbev/pointbev.PNG" alt="pointbev_overview" height="100%" width="100%"/></p> <div class="caption"><b>PointBeV overview.</b> As a sparse method, PointBeV is trained using local predictions, only for sampled 2D points provided as inputs. The points of interest are lifted to form 3D pillars, with each 3D point pulling visual features. To achieve this, PointBeV incorporates an efficient feature extraction process through a Sparse Feature Pulling module, illustrated in the ‘efficient feature extraction’ block. The obtained 3D BeV features are then flattened onto the 2D BeV plane and processed using a sparse U-Net with task-dependent final heads, generating local BeV predictions. For training, we only need sparse signals. At test time, points that have not been sampled are set to zero. </div> <p>PointBeV achieves state-of-the-art results on the nuScenes dataset for vehicle, pedestrian, and lane segmentation, showcasing superior performance in static and temporal settings despite being trained solely with sparse signals. We will release our code along with two new efficient modules used in the architecture: Sparse Feature Pulling, designed for the effective extraction of features from images to BeV, and Submanifold Attention, which enables efficient temporal modeling.</p> <p><img src="/assets/img/posts/2024_cvpr/pointbev_results.PNG" alt="pointbev_results" height="50%" width="50%"/></p> <div class="caption"><b>BeV vehicle IoU vs. memory footprint on nuScenes.</b> The size of a dot represents the number of BeV points being evaluated, the smaller the better. PointBeV has the capacity to explore various trade-offs between efficiency and performance by varying the number of points being considered. The remaining points are considered as zeros in the final prediction. Using PointBeV we can achieve state-of-the-art performance with only a small portion of the points and without losing performance. The memory consumption is calculated using a 40GB A100 GPU. </div> <hr/> <h2 id="dont-drop-your-samples-coherence-aware-training-benefits-conditional-diffusion">Don’t drop your samples! Coherence-aware training benefits Conditional diffusion</h2> <h3 id="highlight">Highlight</h3> <h4 id="authors--nicolas-dufour---victor-besnier-vicky-kalogeiton-david-picard">Authors: <a href="https://nicolas-dufour.github.io/"> Nicolas Dufour </a>, <a href="https://scholar.google.com/citations?hl=fr&amp;user=n_C2h-QAAAAJ">Victor Besnier</a>, <a href="https://vicky.kalogeiton.info/">Vicky Kalogeiton</a>, <a href="https://davidpicard.github.io/">David Picard</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2405.20324">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/nicolas-dufour/CAD">Code</a>] &nbsp;&nbsp; [<a href="https://www.youtube.com/watch?v=4Tu-x2-Zcxs">Video</a>] &nbsp;&nbsp; [<a href="https://nicolas-dufour.github.io/cad.html">Project page</a>]</h4> <p>Conditional diffusion models are powerful generative models that can leverage various types of conditional information, such as class labels, segmentation masks, or text captions. However, in many real-world scenarios, conditional information may be noisy or unreliable due to human annotation errors or weak alignment. In this paper, we propose the Coherence-Aware Diffusion (CAD), a novel method that integrates coherence in conditional information into diffusion models, allowing them to learn from noisy annotations without discarding data. We assume that each data point has an associated coherence score that reflects the quality of the conditional information. We then condition the diffusion model on both the conditional information and the coherence score. In this way, the model learns to ignore or discount the conditioning when the coherence is low. We show that CAD is theoretically sound and empirically effective on various conditional generation tasks. Moreover, we show that leveraging coherence generates realistic and diverse samples that respect conditional information better than models trained on cleaned datasets where samples with low coherence have been discarded.</p> <p><img src="/assets/img/publications/2024_dont_drop/teaser.png" alt="dont_drop_overview" height="90%" width="90%"/></p> <div class="caption"><b>Overview of Don't Drop your Samples.</b> </div> <hr/> <h2 id="supervised-anomaly-detection-for-complex-industrial-images">Supervised Anomaly Detection for Complex Industrial Images</h2> <h4 id="authors--aimira-baitieva--david-hurych--victor-besnier--olivier-bernard">Authors: <a href=""> Aimira Baitieva </a>, <a href="https://scholar.google.com/citations?user=XY1PVwYAAAAJ&amp;hl=fr&amp;oi=ao">David Hurych</a>, <a href="https://scholar.google.com/citations?hl=fr&amp;user=n_C2h-QAAAAJ">Victor Besnier</a>, <a href="">Olivier Bernard</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2405.04953">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/abc-125/segad">Code</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/publications/segad/">Project page</a>]</h4> <p>Automating visual inspection in industrial production lines is essential for increasing product quality across various industries. Anomaly detection (AD) methods serve as robust tools for this purpose. However, existing public datasets primarily consist of images without anomalies, limiting the practical application of AD methods in production settings. To address this challenge, we present (1) the Valeo Anomaly Dataset (VAD), a novel real-world industrial dataset comprising 5000 images, including 2000 instances of challenging real defects across more than 20 subclasses. Acknowledging that traditional AD methods struggle with this dataset, we introduce (2) Segmentation based Anomaly Detector (SegAD). First, SegAD leverages anomaly maps as well as segmentation maps to compute local statistics. Next, SegAD uses these statistics and an optional supervised classifier score as input features for a Boosted Random Forest (BRF) classifier, yielding the final anomaly score. Our SegAD achieves state-of-the-art performance on both VAD (+2.1% AUROC) and the VisA dataset (+0.4% AUROC). The code and the models are publicly available</p> <p><img src="/assets/img/publications/2024_segad/teaser.png" alt="segad_overview" height="100%" width="100%"/></p> <div class="caption"><b>Overview of Supervised Anomaly Detection for Complex Industrial Images</b> </div> <hr/> <h2 id="a-simple-recipe-for-language-guided-domain-generalized-segmentation">A Simple Recipe for Language-guided Domain Generalized Segmentation</h2> <h4 id="authors-mohammad-fahes--tuan-hung-vu--andrei-bursuc--patrick-pérez--raoul-de-charette">Authors: <a href="https://mfahes.github.io/">Mohammad Fahes</a>, <a href="https://tuanhungvu.github.io/">Tuan-Hung Vu</a>, <a href="https://abursuc.github.io/">Andrei Bursuc</a>, <a href="https://ptrckprz.github.io/">Patrick Pérez</a>, <a href="https://team.inria.fr/rits/membres/raoul-de-charette/">Raoul de Charette</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2311.17922">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/astra-vision/FAMix">Code</a>] &nbsp;&nbsp; [<a href="https://www.youtube.com/watch?v=vyjtvx2El9Q">Video</a>] &nbsp;&nbsp; [<a href="https://astra-vision.github.io/FAMix/">page</a>]</h4> <p>Generalization to new domains not seen during training is one of the long-standing goals and challenges in deploying neural networks in real-world applications. Existing generalization techniques necessitate substantial data augmentation, potentially sourced from external datasets, and aim at learning invariant representations by imposing various alignment constraints. Large-scale pretraining has recently shown promising generalization capabilities, along with the potential of bridging different modalities. For instance, the recent advent of vision-language models like CLIP has opened the doorway for vision models to exploit the textual modality. In this paper, we introduce a simple framework for generalizing semantic segmentation networks by employing language as the source of randomization. Our recipe comprises three key ingredients: i) the preservation of the intrinsic CLIP robustness through minimal fine-tuning, ii) language-driven local style augmentation, and iii) randomization by locally mixing the source and augmented styles during training. Extensive experiments report state-of-the-art results on various generalization benchmarks.</p> <p><img src="/assets/img/publications/2024_famix/famix-overview.png" alt="famix_overview" height="100%" width="100%"/></p> <div class="caption"><b>Overall process of FAMix.</b> FAMix consists of two steps. (Left) Local style mining consists of dividing the low-level feature activations into patches, which are used for style mining using Prompt-driven Instance Normalization (PIN). Specifically, for each patch, the dominant class is queried from the ground truth, and the mined style is added to corresponding class-specific style bank. (Right) Training the segmentation network is performed with minimal fine-tuning of the backbone. At each iteration, the low-level feature activations are viewed as grids of patches. For each patch, the dominant class is queried using the ground truth, then a style is sampled from the corresponding style bank. Style randomization is performed by normalizing each patch in the grid by its statistics, and transferring the new style which is a mixing between the original style and the sampled one. The network is trained using only a cross-entropy loss. </div> <p><img src="/assets/img/posts/2024_cvpr/famix_results.PNG" alt="famix_results" height="100%" width="100%"/></p> <div class="caption"><b>Qualitative results.</b> Columns 1-2: Image and ground truth (GT), Columns 3-4-5: Different domain generalization methods, Column 6: Our results. </div> <hr/> <h2 id="make-me-a-bnn-a-simple-strategy-for-estimating-bayesian-uncertainty-from-pre-trained-models">Make Me a BNN: A Simple Strategy for Estimating Bayesian Uncertainty from Pre-trained Models</h2> <h4 id="authors-gianni-franchi--olivier-laurent--maxence-leguéry--andrei-bursuc-andrea-pilzer--angela-yao">Authors: <a href="https://www.ensta-paris.fr/fr/gianni-franchi">Gianni Franchi</a>, <a href="https://scholar.google.com/citations?user=RW4CQ68AAAAJ">Olivier Laurent</a>, <a href="https://scholar.google.com/citations?user=RCUoocYAAAAJ&amp;hl=en">Maxence Leguéry</a>, <a href="https://abursuc.github.io/">Andrei Bursuc</a>, <a href="https://scholar.google.it/citations?user=zooORRsAAAAJ&amp;hl=en">Andrea Pilzer</a>, <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2312.15297">Paper</a>] &nbsp;&nbsp; [<a href="https://torch-uncertainty.github.io/">Code</a>] &nbsp;&nbsp; [<a href="https://www.youtube.com/watch?v=aXqVBAOXc0o">Video</a>] &nbsp;&nbsp; [<a href="https://ensta-u2is-ai.github.io/ABNN-Make-me-a-BNN/">page</a>]</h4> <p>Deep Neural Networks (DNNs) are powerful tools for various computer vision tasks, yet they often struggle with reliable uncertainty quantification — a critical requirement for real-world applications. Bayesian Neural Networks (BNN) are equipped for uncertainty estimation but cannot scale to large DNNs where they are highly unstable to train. To address this challenge, we introduce the Adaptable Bayesian Neural Network (ABNN), a simple and scalable strategy to seamlessly transform DNNs into BNNs in a post-hoc manner with minimal computational and training overheads. ABNN preserves the main predictive properties of DNNs while enhancing their uncertainty quantification abilities through simple BNN adaptation layers (attached to normalization layers) and a few fine-tuning steps on pre-trained models. We conduct extensive experiments across multiple datasets for image classification and semantic segmentation tasks, and our results demonstrate that ABNN achieves state-of-the-art performance without the computational budget typically associated with ensemble methods.</p> <p><img src="/assets/img/posts/2024_cvpr/abnn_overview.PNG" alt="abnn_overview" height="100%" width="100%"/></p> <div class="caption"><b>Illustration of the training process for the ABNN.</b> The procedure begins with training a single DNN $\omega_{\text{MAP}}$, followed by architectural adjustments on the normalization layers to transform it into an ABNN. The final step involves fine-tuning the ABNN model. </div> <hr/> <h2 id="spot-self-training-with-patch-order-permutation-for-object-centric-learning-with-autoregressive-transformers">SPOT: Self-Training with Patch-Order Permutation for Object-Centric Learning with Autoregressive Transformers</h2> <h3 id="highlight-1">Highlight</h3> <h4 id="authors-ioannis-kakogeorgiou-spyros-gidaris-konstantinos-karantzalos---nikos-komodakis">Authors: <a href="https://scholar.google.com/citations?user=B_dKcz4AAAAJ">Ioannis Kakogeorgiou</a>, <a href="https://scholar.google.com/citations?user=7atfg7EAAAAJ&amp;hl=en">Spyros Gidaris</a>, <a href="http://users.ntua.gr/karank/">Konstantinos Karantzalos</a>, <a href="https://www.csd.uoc.gr/~komod/">Nikos Komodakis</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2312.00648">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/gkakogeorgiou/spot">Code</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/publications/spot/">page</a>]</h4> <p>Unsupervised object-centric learning aims to decompose scenes into interpretable object entities, termed slots. Slot-based auto-encoders stand out as a prominent method for this task. Within them, crucial aspects include guiding the encoder to generate object-specific slots and ensuring the decoder utilizes them during reconstruction. This work introduces two novel techniques, (i) an attention-based self-training approach, which distills superior slot-based attention masks from the decoder to the encoder, enhancing object segmentation , and (ii) an innovative patch-order permutation strategy for autoregressive transformers that strengthens the role of slot vectors in reconstruction.</p> <p><img src="/assets/img/posts/2024_cvpr/spot_archi.PNG" alt="spot_archi" height="90%" width="90%"/></p> <div class="caption"> <b>Enhancing unsupervised object-centric learning via self-training.</b> Our two-stage approach starts with exclusive training in the initial stage (not depicted) using the reconstruction loss. In the following stage, shown here, a teacher-student framework is applied. The teacher model, trained in the first stage, guides the student model with an additional loss, distilling attention masks from the teacher’s decoder to the slot-attention masks in the student’s encoder. </div> <p>The effectiveness of these strategies is showcased experimentally. The combined approach significantly surpasses prior slot-based autoencoder methods in unsupervised object segmentation, especially with complex real-world images.</p> <p><img src="/assets/img/publications/2024_spot/spot_visualizations.png" alt="spot_overview" height="100%" width="100%"/></p> <div class="caption"><b>SPOT visualizations.</b> Our novel framework enhances unsupervised object-centric learning in slot-based autoencoders using self-training and sequence permutations in the transformer decoder. It improves object-specific slot generation, excelling in complex real-world images. </div> <hr/> <h2 id="nope-novel-object-pose-estimation-from-a-single-image">NOPE: Novel Object Pose Estimation from a Single Image</h2> <h4 id="authors-van-nguyen-nguyen-thibault-groueix--georgy-ponimatkin--yinlin-hu--renaud-marlet-mathieu-salzmann-vincent-lepetit">Authors: <a href="https://nv-nguyen.github.io/">Van Nguyen Nguyen</a>, <a href="https://imagine.enpc.fr/~groueixt/">Thibault Groueix</a>, <a href="https://scholar.google.co.kr/citations?hl=en&amp;user=5G-6ubcAAAAJ">Georgy Ponimatkin</a>, <a href="https://yinlinhu.github.io/">Yinlin Hu</a>, <a href="http://imagine.enpc.fr/~marletr/">Renaud Marlet</a>, <a href="https://people.epfl.ch/mathieu.salzmann">Mathieu Salzmann</a>, <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2303.13612">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/nv-nguyen/nope">Code</a>] &nbsp;&nbsp; [<a href="https://nv-nguyen.github.io/nope/">page</a>]</h4> <p>TL;DR: We introduce NOPE, a simple approach to estimate relative pose of unseen objects given only a single reference image. NOPE also predicts 3D pose distribution which can be used to address pose ambiguities due to symmetries.</p> <p>The practicality of 3D object pose estimation remains limited for many applications due to the need for prior knowledge of a 3D model and a training period for new objects. To address this limitation, we propose an approach that takes a single image of a new object as input and predicts the relative pose of this object in new images without prior knowledge of the object’s 3D model and without requiring training time for new objects and categories. We achieve this by training a model to directly predict discriminative embeddings for viewpoints surrounding the object. This prediction is done using a simple U-Net architecture with attention and conditioned on the desired pose, which yields extremely fast inference. We compare our approach to state-of-the-art methods and show it outperforms them both in terms of accuracy and robustness.</p> <p><img src="/assets/img/publications/2024_nope/nope.gif" alt="nope_overview" height="55%" width="55%"/></p> <div class="caption"><b>NOPE qualitative results.</b> </div> <hr/> <h2 id="valeo4cast-a-modular-approach-to-end-to-end-forecasting">Valeo4Cast: A Modular Approach to End-to-End Forecasting</h2> <p class="page-description"><a href="https://www.argoverse.org/E2EForecasting.html">Winning solution in Argoverse 2 Unified Detection, Tracking and Forecasting Challenge, at CVPR WAD 2024</a></p> <h4 id="authors-yihong-xu-éloi-zablocki--alexandre-boulch-gilles-puy---mickaël-chen-florent-bartoccioni--nermin-samet---oriane-siméoni---spyros-gidaris---tuan-hung-vu-andrei-bursuc--eduardo-valle-renaud-marlet--matthieu-cord">Authors: <a href="https://scholar.google.fr/citations?user=vMLRRVkAAAAJ">Yihong Xu</a>, <a href="https://scholar.google.fr/citations?user=dOkbUmEAAAAJ">Éloi Zablocki</a>, <a href="https://www.boulch.eu/">Alexandre Boulch</a>, <a href="https://sites.google.com/site/puygilles/home">Gilles Puy</a>, <a href="https://scholar.google.com/citations?user=QnRpMJAAAAAJ">Mickaël Chen</a>, <a href="https://f-barto.github.io/">Florent Bartoccioni</a>, <a href="https://nerminsamet.github.io/">Nermin Samet</a>, <a href="https://osimeoni.github.io/">Oriane Siméoni</a>, <a href="https://scholar.google.fr/citations?user=7atfg7EAAAAJ">Spyros Gidaris</a>, <a href="https://tuanhungvu.github.io/">Tuan-Hung Vu</a>, <a href="https://abursuc.github.io/">Andrei Bursuc</a>, <a href="https://scholar.google.com/citations?user=lxWPqWAAAAAJ">Eduardo Valle</a>, <a href="http://imagine.enpc.fr/~marletr/">Renaud Marlet</a>, <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2406.08113">Paper</a>] &nbsp;&nbsp; [<a href="https://eval.ai/web/challenges/challenge-page/2006/leaderboard/4752">leaderboard</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/publications/valeo4cast/">page</a>]</h4> <p>Motion forecasting is crucial in autonomous driving systems to anticipate the future trajectories of surrounding agents such as pedestrians, vehicles, and traffic signals. In end-to-end forecasting, the model must jointly detect from sensor data (cameras or LiDARs) the position and past trajectories of the different elements of the scene and predict their future location. We depart from the current trend of tackling this task via end-to-end training from perception to forecasting and we use a modular approach instead. Following a recent study, we individually build and train detection, tracking, and forecasting modules. We then only use consecutive finetuning steps to integrate the modules better and alleviate compounding errors. Our study reveals that this simple yet effective approach significantly improves performance on the end-to-end forecasting benchmark. Consequently, our solution ranks first in the Argoverse 2 end-to-end Forecasting Challenge held at CVPR 2024 Workshop on Autonomous Driving (WAD), with 63.82 mAPf. We surpass forecasting results by +17.1 points over last year’s winner and by +13.3 points over this year’s runner-up. This remarkable performance in forecasting can be explained by our modular paradigm, which integrates finetuning strategies and significantly outperforms the end-to-end-trained counterparts.</p> <p><img src="/assets/img/publications/2024_valeo4cast/valeo4cast.PNG" alt="valeo4cast_overview" height="100%" width="100%"/></p> <div class="caption"><b>Valeo4Cast overview.</b> </div> <hr/> <h2 id="occfeat-self-supervised-occupancy-feature-prediction-for-pretraining-bev-segmentation-networks">OccFeat: Self-supervised Occupancy Feature Prediction for Pretraining BEV Segmentation Networks</h2> <p class="page-description"><a href="https://cvpr2024.wad.vision/">CVPR 2024 Workshop on Autonomous Driving (WAD)</a></p> <h4 id="authors-sophia-sirko-galouchenko-alexandre-boulch--spyros-gidaris--andrei-bursuc---antonin-vobecky---renaud-marlet--patrick-pérez">Authors: Sophia Sirko-Galouchenko, <a href="https://www.boulch.eu/">Alexandre Boulch</a>, <a href="https://scholar.google.fr/citations?user=7atfg7EAAAAJ&amp;hl=en">Spyros Gidaris</a>, <a href="https://abursuc.github.io/">Andrei Bursuc</a>, <a href="https://vobecant.github.io/">Antonin Vobecky</a>, <a href="http://imagine.enpc.fr/~marletr/">Renaud Marlet</a>, <a href="https://ptrckprz.github.io/">Patrick Pérez</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2404.14027">Paper</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/publications/occfeat/">page</a>]</h4> <p>We introduce a self-supervised pretraining method, called OccFeat, for camera-only Bird’s-Eye-View (BEV) segmentation networks. With OccFeat, we pretrain a BEV network via occupancy prediction and feature distillation tasks. Occupancy prediction provides a 3D geometric understanding of the scene to the model. However, the geometry learned is class-agnostic. Hence, we add semantic information to the model in the 3D space through distillation from a self-supervised pretrained image foundation model. Models pretrained with our method exhibit improved BEV semantic segmentation performance, particularly in low-data scenarios. Moreover, empirical results affirm the efficacy of integrating feature distillation with 3D occupancy prediction in our pretraining approach.</p> <p><img src="/assets/img/publications/2024_occfeat/occfeat_teaser.png" alt="occfeat_overview" height="100%" width="100%"/></p> <div class="caption"><b>Overview of OccFeat’s self-supervised BEV pretraining approach.</b> OccFeat attaches an auxiliary pretraining head on top of the BEV network. This head “unsplats” the BEV features to a 3D feature volume and predicts with it (a) the 3D occupancy of the scene (occupancy reconstruction loss) and (b) high-level self-supervised image features characterizing the occupied voxels (occupancy-guided distillation loss). The occupancy targets are produced by “voxelizing” Lidar points, while the self-supervised image foundation model DINOv2 provides the feature targets for the occupied voxels. The pretraining head is removed after the pretraining. </div> <p>The results show the benefit of our pretraining method, especially in low-shot regimes, e.g., when using annotations only for 1% or 10% of nuScene’s training data. Additionally, our OccFeat pretraining improves the robustness, as evaluated on the nuScenes-C benchmark.</p> <p><img src="/assets/img/posts/2024_cvpr/occfeat_results.PNG" alt="occfeat_results" height="100%" width="100%"/></p> <div class="caption"> Performance comparison in low data regime 1% annotated data of nuScenes (Left). Study on robustness. Segmentation results on nuScenes-C dataset for Vehicle classes using BEVFormer network with EN-B0 image backbone on 100% annotated data. Comparison of our OccFeat against no BEV pretraining (Right). </div> <hr/> <h2 id="what-makes-multimodal-in-context-learning-work">What Makes Multimodal In-Context Learning Work?</h2> <p class="page-description"><a href="https://prompting-in-vision.github.io/index_cvpr24.html">CVPR 2024 Workshop on Prompting in Vision</a></p> <h4 id="authors-folco-bertini-baldassini--mustafa-shukor-matthieu-cord-laure-soulier--benjamin-piwowarski">Authors: <a href="https://www.folbaeni.com/">Folco Bertini Baldassini</a>, <a href="https://scholar.google.com/citations?user=lhp9mRgAAAAJ&amp;hl=en">Mustafa Shukor</a>, <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a>, <a href="https://pages.isir.upmc.fr/soulier/">Laure Soulier</a>, <a href="https://www.piwowarski.fr/">Benjamin Piwowarski</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2404.15736">Paper</a>] &nbsp;&nbsp; [<a href="https://gitlab.com/folbaeni/multimodal-icl">code</a>] &nbsp;&nbsp; [<a href="https://multimodal-icl-folbaeni-1988a753e0abbbc71bb3967331bb69edafda92e.gitlab.io/">page</a>]</h4> <p>Large Language Models have demonstrated remarkable performance across various tasks, exhibiting the capacity to swiftly acquire new skills, such as through In-Context Learning (ICL) with minimal demonstration examples. In this work, we present a comprehensive framework for investigating Multimodal ICL (M-ICL) in the context of Large Multimodal Models. We consider the best open-source multimodal models (e.g., IDEFICS, OpenFlamingo) and a wide range of multimodal tasks. Our study unveils several noteworthy findings: (1) M-ICL primarily relies on text-driven mechanisms, showing little to no influence from the image modality. (2) When used with advanced-ICL strategy (like RICES), M-ICL is not better than a simple strategy based on majority voting over context examples. Moreover, we identify several biases and limitations of M-ICL that warrant consideration prior to deployment.</p> <p><img src="/assets/img/publications/2024_multimodal_icl/multimodal-icl.PNG" alt="icl_overview" height="60%" width="60%"/></p> <div class="caption"><b>Empirical analysis of Multimodal In-Context Learning (M-ICL) behavior.</b> </div>]]></content><author><name></name></author><category term="3d perception"/><category term="multi-sensor"/><category term="limited supervision"/><category term="reliability"/><category term="motion forecasting"/><category term="driving"/><summary type="html"><![CDATA[Gilles Puy, Loïck Chambon, Victor Besnier, Sophia Sirko-Galouchenko, Spyros Gidaris, Alexandre Boulch, Renaud Marlet, Andrei Bursuc, Éloi Zablocki]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io//assets/img/posts/2024_cvpr/cvpr_banner.PNG"/><media:content medium="image" url="https://valeoai.github.io//assets/img/posts/2024_cvpr/cvpr_banner.PNG" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">valeo.ai at NeurIPS 2023</title><link href="https://valeoai.github.io//posts/neurips-2023" rel="alternate" type="text/html" title="valeo.ai at NeurIPS 2023"/><published>2023-12-08T00:00:00+00:00</published><updated>2023-12-08T00:00:00+00:00</updated><id>https://valeoai.github.io//posts/valeoai-at-neurips-2023</id><content type="html" xml:base="https://valeoai.github.io//posts/neurips-2023"><![CDATA[<p>The <a href="https://neurips.cc/">Neural Information Processing Systems Conference (NeurIPS)</a> is a major inter-disciplinary event that brings together researchers and practicioners in machine learning, computer vision, natural language processing, optimization, statistics, but also neuroscience, natural sciences, social sciences, etc. This year, at the thirty-seventh edition of NeurIPS, the <a href="../">valeo.ai</a> team will present 4 papers in the main conference and 1 in the workshops.</p> <p>Notably, we explore perception via different sensors, e.g., audio, on the path towards increasingly autonomous systems. We also study the interaction between different sensing modalities (images, language, Lidar point clouds) and advance a tri-modal self-supervised learning algorithm for 3D semantic voxel occupancy prediction from a rig of cameras mounted on a vehicle. We further show how to obtain robust deep models starting from pre-trained foundation models finetuned with reinforcement learning from human feedback. Finally, we analyze different generative models (diffusion models, GANs) and advance a unification framework considering them as instances of Particle Models.</p> <p>We will be happy to discuss more about these projects and ideas, and share our exciting ongoing research. Take a quick view of our papers below and come meet us at the posters or catch us for a coffee in the hallways.</p> <h2 id="resilient-multiple-choice-learning-a-learned-scoring-scheme-with-application-to-audio-scene-analysis">Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis</h2> <h4 id="authors-victor-letzelter-mathieu-fontaine-mickaël-chen-patrick-pérez-slim-essid-gaël-richard">Authors: Victor Letzelter, Mathieu Fontaine, Mickaël Chen, Patrick Pérez, Slim Essid, Gaël Richard</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2311.01052">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/Victorletzelter/code-rMCL">Code</a>] &nbsp;&nbsp; [<a href="../publications/rmcl/">Project page</a>]</h4> <p>In this work, we tackle ambiguous machine learning tasks, where single predictions don’t suffice due to the task’s nature or inherent uncertainties. We introduce a robust multi-hypotheses framework that is capable of deterministically offering a range of plausible predictions at inference time. Our experiments on both synthetic data and real-world audio data affirm the potential and versatility of our method. Check out the paper and the code for more details.</p> <p><img src="/assets/img/posts/2023_neurips/training_dynamics.gif" alt="rmcl_overview" height="100%" width="100%"/></p> <p>This problem involves estimating a conditional distribution that is dependent on the input. The accompanying animation illustrates the early stages in the evolution of our model’s learning process, highlighting how it progressively refines its predictions (represented by shaded blue points) to the actual data distribution (indicated by green points), which varies with the input ‘t’.</p> <hr/> <h2 id="pop-3d-open-vocabulary-3d-occupancy-prediction-from-images">POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images</h2> <h4 id="authors-antonin-vobecky-oriane-siméoni-david-hurych-spyros-gidaris-andrei-bursuc-patrick-pérez-josef-sivic">Authors: Antonin Vobecky, Oriane Siméoni, David Hurych, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Josef Sivic</h4> <h4 align="center"> [<a href="https://openreview.net/forum?id=eBXM62SqKY">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/vobecant/POP3D">Code</a>] &nbsp;&nbsp; [<a href="https://vobecant.github.io/POP3D">Project page</a>]</h4> <p>POP-3D is an approach to predict open-vocabulary 3D semantic voxel occupancy map from input 2D images to enable 3D grounding, segmentation, and retrieval of free-form language queries.</p> <p><img src="/assets/img/posts/2023_neurips/pop3d-overview.png" alt="pop3d_overview" height="100%" width="100%"/></p> <div class="caption">Given surround-view images on the input, our POP-3D outputs voxel occupancy with 3D-language features, which one can query using text, e.g., to obtain zero-shot semantic segmentation. </div> <p>We design a new model architecture for open-vocabulary 3D semantic occupancy prediction. The architecture consists of a 2D-3D encoder together with occupancy prediction and 3D-language heads. The output is a dense voxel map of 3D grounded language embeddings enabling a range of open-vocabulary tasks. Next, we develop a tri-modal self-supervised learning algorithm that leverages three modalities: (i) images, (ii) language, and (iii) LiDAR point clouds and enables training the proposed architecture using a strong pre-trained vision-language model without the need for any 3D manual language annotations.</p> <p><img src="/assets/img/posts/2023_neurips/pop3d-model.png" alt="pop3d_model" height="100%" width="100%"/></p> <div class="caption">Overview of POP-3D architecture and training approach.</div> <p>Finally, we demonstrate the strengths of the proposed model quantitatively on several open-vocabulary tasks: Zero-shot 3D semantic segmentation using existing datasets; 3D grounding, and retrieval of free-form language queries, using a small dataset that we propose as an extension of nuScenes.</p> <p><img src="/assets/img/posts/2023_neurips/pop3d-qualitative.png" alt="pop3d_example" height="100%" width="100%"/></p> <hr/> <h2 id="rewarded-soups-towards-pareto-optimal-alignment-by-interpolating-weights-fine-tuned-on-diverse-rewards">Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards</h2> <h4 id="authors-alexandre-ramé-guillaume-couairon-mustafa-shukor-corentin-dancette-jean-baptiste-gaya-laure-soulier-matthieu-cord">Authors: Alexandre Ramé, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste Gaya, Laure Soulier, Matthieu Cord</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2306.04488">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/alexrame/rewardedsoups">Code</a>] &nbsp;&nbsp; [<a href="https://huggingface.co/spaces/alexrame/rewardedsoups">Project page</a>]</h4> <p>Foundation models are first pre-trained on vast unsupervised datasets and then fine-tuned on labeled data. Reinforcement learning, notably from human feedback (RLHF), can further align the network with the intended usage. Yet the imperfections in the proxy reward may hinder the training and lead to suboptimal results; the diversity of objectives in real-world tasks and human opinions exacerbate the issue. This paper proposes embracing the heterogeneity of diverse rewards by following a multi-policy strategy. Rather than focusing on a single a priori reward, we aim for Pareto-optimal generalization across the entire space of preferences. To this end, we propose rewarded soup, first specializing multiple networks independently (one for each proxy reward) and then interpolating their weights linearly. This succeeds empirically because we show that the weights remain linearly connected when fine-tuned on diverse rewards from a shared pre-trained initialization. We demonstrate the effectiveness of our approach for text-to-text (summarization, Q&amp;A, helpful assistant, review), text-image (image captioning, text-to-image generation, visual grounding, VQA), and control (locomotion) tasks. We hope to enhance the alignment of deep models, and how they interact with the world in all its diversity.</p> <p><img src="/assets/img/posts/2023_neurips/rewarded-soups.png" alt="rs_overview" height="100%" width="100%"/></p> <div class="caption"><b>Illustration of the different steps of our proposed rewarded soup (RS).</b> After unsupervised pre-training and supervised fine-tuning, we launch $N$ independent RL fine-tunings on the proxy rewards $\{R_i\}^{N}_{i=1}$. Then we combine the trained networks by interpolation in the weight space. The final weights are adapted at test time by selecting the coefficient $\lambda$.</div> <hr/> <h2 id="unifying-gans-and-score-based-diffusion-as-generative-particle-models">Unifying GANs and Score-Based Diffusion as Generative Particle Models</h2> <h4 id="authors-jean-yves-franceschi-mike-gartrell-ludovic-dos-santos-thibaut-issenhuth-emmanuel-de-bézenac-mickaël-chen-alain-rakotomamonjy">Authors: Jean-Yves Franceschi, Mike Gartrell, Ludovic Dos Santos, Thibaut Issenhuth, Emmanuel de Bézenac, Mickaël Chen, Alain Rakotomamonjy</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2305.16150">Paper</a>] &nbsp;&nbsp; [Code (coming soon)]</h4> <p>By describing the trajectories of GAN outputs during training with particle evolution equations, we propose an unifying framework for GAN and Diffusion Models. We provide a new insights on the role of the generator network, and as proof of concept validating our theories, we propose methods to train a generator with score-based gradient instead of a discriminator, or to use a discriminator’s gradient flow to generate instead of training a generator.</p> <p><img src="/assets/img/posts/2023_neurips/unify-gan.png" alt="unigan_overview" height="70%" width="70%"/></p> <hr/> <h2 id="evaluating-the-structure-of-cognitive-tasks-with-transfer-learning">Evaluating the structure of cognitive tasks with transfer learning</h2> <p class="page-description"><a href="https://ai4sciencecommunity.github.io/neurips23.html">NeurIPS Workshop on AI for Scientific Discovery: From Theory to Practice</a></p> <h4 id="authors-bruno-aristimunha-raphael-y-de-camargo-walter-h-lopez-pinaya-sylvain-chevallier-alexandre-gramfort-cedric-rommel">Authors: Bruno Aristimunha, Raphael Y. de Camargo, Walter H. Lopez Pinaya, Sylvain Chevallier, Alexandre Gramfort, Cedric Rommel</h4> <h4 align="center"> [<a href="https://cedricrommel.github.io/assets/pdfs/NeurIPS_2023_AI_for_Science_Workshop.pdf">Paper</a>] &nbsp;&nbsp; [Code (coming soon)]</h4> <p>Electroencephalography (EEG) decoding is a challenging task due to the limited availability of labeled data. While transfer learning is a promising technique to address this challenge, it assumes that transferable data domains and tasks are known, which is not the case in this setting. This work investigates the transferability of deep learning representations between different EEG decoding tasks.</p> <p><img src="/assets/img/posts/2023_neurips/eval-cog-tasks.png" alt="cog_overview" height="90%" width="90%"/></p> <div class="caption"><b>Learned transferability maps for both datasets.</b> Each node corresponds to a distinct cognitive task. Arrow width represents the average transfer performance when using the representations learned from a source task to decode a target task.</div> <p>We conduct extensive experiments using state-of-the-art decoding models on two recently released EEG datasets, ERPCore and M3CV, containing over 140 subjects and 11 distinct cognitive tasks.</p> <p>From an EEG processing perspective, our results can be used to leverage related datasets for alleviating EEG data scarcity with transfer learning. We show that even with a linear probing transfer method, we are able to boost by up to 28% the performance of some tasks. From a neuroscientific standpoint, our transfer maps provide insights into the hierarchical relations between cognitive tasks, hence enhancing our understanding of how these tasks are connected. We discover for example evidence that certain decoding paradigms elicit specific and narrow brain activities, while others benefit from pre-training on a broad range of representations.</p>]]></content><author><name></name></author><category term="multi-sensor"/><category term="limited supervision"/><category term="reliability"/><category term="deep-learning"/><summary type="html"><![CDATA[Victor Letzelter, Antonin Vobecky, Mickael Chen, Cedric Rommel, Matthieu Cord, Andrei Bursuc]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io//assets/img/posts/2023_neurips/logo_neurips.svg"/><media:content medium="image" url="https://valeoai.github.io//assets/img/posts/2023_neurips/logo_neurips.svg" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">valeo.ai at ICCV 2023</title><link href="https://valeoai.github.io//posts/iccv-2023" rel="alternate" type="text/html" title="valeo.ai at ICCV 2023"/><published>2023-09-26T00:00:00+00:00</published><updated>2023-09-26T00:00:00+00:00</updated><id>https://valeoai.github.io//posts/valeoai-at-iccv-2023</id><content type="html" xml:base="https://valeoai.github.io//posts/iccv-2023"><![CDATA[<p>The <a href="https://iccv2023.thecvf.com/">IEEE / CVF International Conference on Computer Vision (ICCV)</a> is a landmark event for the increasingly large and diverse community of researchers in computer vision and machine learning. This year, ICCV takes place in Paris, home of the <a href="../">valeo.ai</a> team. From interns to senior researchers, the valeo.ai team will participate in mass at ICCV and will be looking forward to welcoming you and talking about the exciting progress and ideas in the field.</p> <p>At ICCV 2023 we will present 5 papers in the main conference and 3 in the workshops. We are also organizing 2 tutorials with 2 challenges (<a href="https://valeoai.github.io/bravo/">BRAVO</a> and <a href="https://uncv2023.github.io/">UNCV</a>) and a tutorial (<a href="https://abursuc.github.io/many-faces-reliability/">Many Faces of Reliability</a>). Take a quick view of our papers in the conference and come meet us at the posters, at our booth or in the hallway.</p> <h2 id="using-a-waffle-iron-for-automotive-point-cloud-semantic-segmentation">Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation</h2> <h4 id="authors-gilles-puy-alexandre-boulch-renaud-marlet">Authors: Gilles Puy, Alexandre Boulch, Renaud Marlet</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2301.10100">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/WaffleIron">Code</a>] &nbsp;&nbsp; [<a href="../publications/waffleiron/">Project page</a>]</h4> <p>Semantic segmentation of point clouds delivered by lidars permits autonomous vehicles to make sense of their 3D surrounding environment. Sparse convolutions have become a de-facto tool to process these large outdoor point clouds. The top performing methods on public benchmarks, such SemanticKITTI or nuScenes, all leverage sparse convolutions. Nevertheless, despite their undeniable success and efficiency, these convolutions remain available in a limited number of deep learning frameworks and hardware platforms. In this work, we propose an alternative backbone built with tools broadly available (such as 2D and 1D convolutions) but that still reaches the level of performance of the top methods on automotive datasets.</p> <p>We propose a point-based backbone, called WaffleIron, which is essentially built using standard MLPs and dense 2D convolutions, both readily available in all deep learning frameworks thanks to their wide use in the field of computer vision. The architecture of this backbone is illustrated in the figure below. It is inspired by the recent MLP-Mixer. It takes as input a point cloud with a token associated to each point. All these point tokens are then updated by a sequence of layers, each containing a token-mixing step (made of dense 2D convolutions) and a channel-mixing step (made of a MLP shared across points).</p> <p><img src="/assets/img/posts/2023_iccv/waffleiron.png" alt="waffle_overview" height="70%" width="70%"/></p> <div class="caption">The WaffleIron backbone takes as input point tokens, provided by an embedding layer (not represented), and updates these point representations L times via a point token-mixing layer (containing the WI block) followed by a channel-mixing layer. The WI block consists of a 2D projection along one of the main axes, a feed-forward network (FFN) with two dense channel-wise 2D convolutions with a ReLU activation in the hidden layer, and a simple copy of the 2D features to the 3D points. The channel-mixing layer contains a batch-norm, a MLP shared across each point, and a residual connection. The WaffleIron backbone is free of any point downsampling or upsampling layer, farthest point sampling, nearest neighbor search, or sparse convolution. </div> <p>WaffleIron has three main hyperparameters to tune: the depth L, the width F and the resolution of the 2D grid. We show that these parameters are easy to tune: the performance increases with the network width F and depth L, until an eventual saturation; we observe stable results over a wide range of values for the resolution of the 2D grid.</p> <p>In our paper, we also provide many details on how to train WaffleIron to reach the performance of top-entries on two autonomous driving benchmarks: SemanticKITTI and nuScenes.</p> <hr/> <h2 id="pøda-prompt-driven-zero-shot-domain-adaptation">PØDA: Prompt-driven Zero-shot Domain Adaptation</h2> <h4 id="authors-mohammad-fahes-tuan-hung-vu-andrei-bursuc-patrick-pérez-raoul-de-charette">Authors: Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2212.03241">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/astra-vision/PODA">Code</a>] &nbsp;&nbsp; [<a href="https://www.youtube.com/watch?v=kataxQoPuSE">Video</a>] &nbsp;&nbsp; [<a href="https://astra-vision.github.io/PODA/">Project page</a>]</h4> <p>Domain adaptation has been vastly investigated in computer vision but still requires access to target images at train time, which might be intractable in some uncommon conditions. In this paper, we propose the task of ‘Prompt-driven Zero-shot Domain Adaptation’, where we adapt a model trained on a source domain using only a general description in natural language of the target domain, i.e., a prompt. First, we leverage a pre-trained contrastive vision-language model (CLIP) to optimize affine transformations of source features, steering them towards the target text embedding while preserving their content and semantics. To achieve this, we propose Prompt-driven Instance Normalization (PIN). Second, we show that these prompt-driven augmentations can be used to perform zero-shot domain adaptation for semantic segmentation. Experiments demonstrate that our method significantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand, even surpassing one-shot unsupervised domain adaptation. A similar boost is observed on object detection and image classification</p> <p><img src="/assets/img/posts/2023_iccv/poda.png" alt="poda_overview" height="80%" width="80%"/></p> <div class="caption">We perform zero-shot adaptation with natural language prompts. PØDA enables the adaptation of a segmenter model (here, DeepLabv3+ trained on the source dataset Cityscapes) to unseen conditions with only a prompt. Source-only predictions are shown as smaller segmentation masks to the left or right of the test images. </div> <hr/> <h2 id="you-never-get-a-second-chance-to-make-a-good-first-impression-seeding-active-learning-for-3d-semantic-segmentation">You Never Get a Second Chance To Make a Good First Impression: Seeding Active Learning for 3D Semantic Segmentation</h2> <h4 id="authors-nermin-samet-oriane-siméoni-gilles-puy-georgy-ponimatkin-renaud-marlet-vincent-lepetit">Authors: Nermin Samet, Oriane Siméoni, Gilles Puy, Georgy Ponimatkin, Renaud Marlet, Vincent Lepetit</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2304.11762">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/nerminsamet/seedal">Code</a>]</h4> <p>We are interested in the efficient annotation of sparse 3D point clouds (as captured indoors by depth cameras or outdoors by automotive lidars) for semantic segmentation. Active Learning (AL) iteratively selects relevant data fractions to annotate within a given budget, but requires a first fraction of the dataset (a ’seed’) to be already annotated to estimate the benefit of annotating other data fractions. We show that the choice of the seed can significantly affect the performance of many AL methods and propose a method, named SeedAL, for automatically constructing a seed that will ensure good performance for AL. Assuming that images of the point clouds are available, which is common, our method relies on powerful unsupervised image features to measure the diversity of the point clouds. It selects the point clouds for the seed by optimizing the diversity under an annotation budget, which can be done by solving a linear optimization problem. Our experiments demonstrate the effectiveness of our approach compared to random seeding and existing methods on both the S3DIS and SemanticKitti datasets.</p> <p><img src="/assets/img/posts/2023_iccv/seedal.png" alt="seedal_overview" height="70%" width="70%"/></p> <div class="caption"><b>Impact of active learning seed on performance. </b>We show the variability of results obtained with 20 different random seeds (blue dashed lines), within an initial annotation budget of 3% of the dataset, when using various active learning methods for 3D semantic segmentation of S3DIS. We compare it to the result obtained with our seed selection strategy (solid red line), named SeedAL, which performs better or on par with the best (lucky) random seeds among 20, and “protects” from very bad (unlucky) random seeds.</div> <hr/> <h2 id="ep-alm-efficient-perceptual-augmentation-of-language-models">eP-ALM: Efficient Perceptual Augmentation of Language Models</h2> <h4 id="authors-mustafa-shukor-corentin-dancette-matthieu-cord">Authors: Mustafa Shukor, Corentin Dancette, Matthieu Cord</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2303.11403">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/mshukor/eP-ALM">Code</a>] &nbsp;&nbsp; [<a href="https://mshukor.github.io/eP-ALM.github.io/">Project page</a>]</h4> <p>eP-ALM aims to augment large language models (LLMs) with perception. While most existing approaches train a large number of parameters and rely on extensive multimodal pre-training, we investigate the minimal computational effort required to adapt unimodal models to multimodal tasks. We show that by freezing more than 99% of total parameters, training only one linear projection layer and prepending only one trainable token, our approach (dubbed eP-ALM) significantly outperforms other baselines on VQA and captioning for image, video and audio modalities.</p> <p><img src="/assets/img/posts/2023_iccv/ep-alm.png" alt="epalm_overview" height="70%" width="70%"/></p> <div class="caption"><b>Illustration of the adaptation mechanism in eP-ALM.</b> The perceptual input (image/video/audio) is fed to the perceptual encoder E (e.g., ViT) and the corresponding text to the LM (e.g., OPT), which then generates a text conditioned on the perceptual input. The multimodal interaction is done via the [CLS] tokens acting as Perceptual Prompt, and are extracted from the last layers of the encoder, then injected in the last layers of LM, after passing by the Linear Connection C. The previous [CLS] token is replaced by the new one coming from a deeper layer, keeping the number of tokens fixed. The first layers (grayed) of each model are kept intact without any modality interaction. We ease the adaptation with a Soft Prompt that is prepended to the input of LM. </div> <hr/> <h2 id="zero-shot-spatial-layout-conditioning-for-text-to-image-diffusion-models">Zero-shot spatial layout conditioning for text-to-image diffusion models</h2> <h4 id="authors-guillaume-couairon-marlène-careil-matthieu-cord-stéphane-lathuilière-jakob-verbeek">Authors: Guillaume Couairon, Marlène Careil, Matthieu Cord, Stéphane Lathuilière, Jakob Verbeek</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2306.13754">Paper</a>]</h4> <p>Large-scale text-to-image diffusion models have considerably improved the state of the art in generative image modeling, and provide an intuitive and powerful user interface to drive the image generation process. In this paper, we propose ZestGuide, a “zero-shot” segmentation guidance approach that can be integrated into pre-trained text-image diffusion models, and requires no additional training. It exploits the implicit segmentation maps that can be extracted from cross-attention layers, and uses them to align generation with input masks.</p> <p><img src="/assets/img/posts/2023_iccv/zest-guide.png" alt="zest_overview" height="70%" width="70%"/></p> <div class="caption">ZestGuide generates images conditioned on segmentation maps with corresponding free-form textual descriptions. </div> <hr/> <h2 id="diffhpe-robust-coherent-3d-human-pose-lifting-with-diffusion">DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion</h2> <p class="page-description"><a href="https://web.northeastern.edu/smilelab/amfg2023/">ICCV Workshop on Analysis and Modeling of Faces and Gestures</a></p> <h4 id="authors-cédric-rommel-eduardo-valle-mickaël-chen-souhaiel-khalfaoui-renaud-marlet-matthieu-cord-patrick-pérez">Authors: Cédric Rommel, Eduardo Valle, Mickaël Chen, Souhaiel Khalfaoui, Renaud Marlet, Matthieu Cord, Patrick Pérez</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2309.01575">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/diffhpe">Code</a>] &nbsp;&nbsp; [<a href="../publications/diffhpe">Project page</a>]</h4> <p>Diffusion models are making waves across various domains, including computer vision, natural language processing and time-series analysis. However, its application to purely predictive tasks, such as 3D human pose estimation (3D-HPE), remains largely unexplored. While a few pioneering works have shown promising performance metrics in 3D-HPE, the understanding of the benefits of diffusion models over classical supervision — as well as key design choices — is still in its infancy. In this work, we address those concerns, providing an in-depth analysis of the effects of diffusion models on 3D-HPE.</p> <p><img src="/assets/img/posts/2023_iccv/diffhpe.gif" alt="diffhpe_overview" height="100%" width="100%"/></p> <div class="caption">Poses across the learned reverse diffusion process converge to an accurate 3D reconstruction of the corresponding 2D pose in pixel space.</div> <p>More precisely, we propose DiffHPE, a novel strategy to use diffusion models in 3D-HPE, and show that combining diffusion with pre-trained supervised models allows to outperform both pure diffusion and pure supervised models trained separately. Our analysis demonstrates not only that the diffusion framework can be used to enhance accuracy, as previously understood, but also that it can improve robustness and coherence. Namely, our experiments showcase how poses estimated with diffusion models’ display better bilateral and temporal coherence, and are more robust to occlusions, even when not perfectly trained for the latter.</p> <hr/> <h2 id="challenges-of-using-real-world-sensory-inputs-for-motion-forecasting-in-autonomous-driving">Challenges of Using Real-World Sensory Inputs for Motion Forecasting in Autonomous Driving</h2> <p class="page-description"><a href="https://sites.google.com/view/road-plus-plus">ROAD++: The Second Workshop and Challenge on Event Detection for Situation Awareness in Autonomous Driving</a></p> <h4 id="authors-yihong-xu-loïck-chambon-éloi-zablocki-mickaël-chen-matthieu-cord-patrick-pérez">Authors: Yihong Xu, Loïck Chambon, Éloi Zablocki, Mickaël Chen, Matthieu Cord, Patrick Pérez</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2306.09281">Paper</a>] &nbsp;&nbsp; [<a href="../publications/real-world-forecasting/">Project page</a>]</h4> <p>Motion forecasting is crucial in enabling autonomous vehicles to anticipate the future trajectories of surrounding agents. To do so, it requires solving mapping, detection, tracking, and then forecasting problems, in a multi-step pipeline. In this complex system, advances in conventional forecasting methods have been made using curated data, i.e., with the assumption of perfect maps, detection, and tracking. This paradigm, however, ignores any errors from upstream modules. Meanwhile, an emerging end-to-end paradigm, that tightly integrates the perception and forecasting architectures into joint training, promises to solve this issue. So far, however, the evaluation protocols between the two methods were incompatible and their comparison was not possible. In fact, and perhaps surprisingly, conventional forecasting methods are usually not trained nor tested in real-world pipelines (e.g., with upstream detection, tracking, and mapping modules). In this work, we aim to bring forecasting models closer to real-world deployment. First, we propose a unified evaluation pipeline for forecasting methods with real-world perception inputs, allowing us to compare the performance of conventional and end-to-end methods for the first time. Second, our in-depth study uncovers a substantial performance gap when transitioning from curated to perception-based data. In particular, we show that this gap (1) stems not only from differences in precision but also from the nature of imperfect inputs provided by perception modules, and that (2) is not trivially reduced by simply finetuning on perception outputs. Based on extensive experiments, we provide recommendations for critical areas that require improvement and guidance towards more robust motion forecasting in the real world. We will release an evaluation library to benchmark models under standardized and practical conditions.</p> <p><img src="/assets/img/posts/2023_iccv/e2e_forecasting.png" alt="forecast_overview" height="90%" width="90%"/></p> <div class="caption"><b>Study overview.</b> We study the challenges of deploying motion forecasting models into the real world when only predicted perception inputs are available. We compare: (1) (top) "conventional methods" (i.e., methods trained on curated input data) where (middle) we directly replace the curated inputs with real-world data, and (2) (bottom) "end-to-end methods" that are trained and used with perception modules. In the real-world setting, evaluation is challenging as the past tracks are estimated with arbitrary identities, making it difficult to establish a direct correspondence to GT identities. Therefore, we propose a matching process (purple) to assign predictions to GT and thus evaluate forecasting performances. Moreover, we study in depth the impact changing from curated data (green) to real-world (orange) mapping, or detection and tracking errors to motion forecasting. </div> <hr/> <h2 id="pop-3d-open-vocabulary-3d-occupancy-prediction-from-images">POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images</h2> <p class="page-description"><a href="https://opensun3d.github.io/">ICCV 2023 Workshop on Open-Vocabulary 3D Scene Understanding (OpenSUN 3D)</a></p> <h4 id="authors-antonin-vobecky-oriane-siméoni-david-hurych-spyros-gidaris-andrei-bursuc-patrick-pérez-josef-sivic">Authors: Antonin Vobecky, Oriane Siméoni, David Hurych, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Josef Sivic</h4> <h4 align="center"> [<a href="https://data.ciirc.cvut.cz/public/projects/2023POP3D/resources/pop3d_paper.pdf">Paper</a>]</h4> <p>We propose an approach to predict a 3D semantic voxel occupancy map from input 2D images with features allowing 3D grounding, segmentation and retrieval of free-form language queries. To this end: We design a new architecture that consists of a 2D-3D encoder together with occupancy prediction and 3D-language heads; We develop a tri-modal self-supervised training that leverages three modalities – images, language and LiDAR point clouds– and enables learning the proposed architecture using a strong pre-trained vision-language model without the need for any 3D manual annotations. We quantitatively evaluate the proposed model on the task of zero-shot 3D semantic segmentation using existing datasets and show results on the tasks of 3D grounding and retrieval of free-form language queries.</p> <p><img src="/assets/img/posts/2023_iccv/pop3d.png" alt="forecast_overview" height="100%" width="100%"/></p> <div class="caption"><b>Method overview.</b>Given surround-view images, POP-3D produces a voxel grid of text-aligned features that support open-vocabulary downstream tasks such as zero-shot occupancy segmentation or text-based grounding and retrieval. </div>]]></content><author><name></name></author><category term="3d perception"/><category term="multi-sensor"/><category term="limited supervision"/><category term="reliability"/><category term="domain-adaptation"/><summary type="html"><![CDATA[Gilles Puy, Tuan-Hung Vu, Oriane Siméoni, Matthieu Cord, Cédric Rommel, Andrei Bursuc]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io//assets/img/posts/2023_iccv/iccv_logo.svg"/><media:content medium="image" url="https://valeoai.github.io//assets/img/posts/2023_iccv/iccv_logo.svg" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">valeo.ai at CVPR 2023</title><link href="https://valeoai.github.io//posts/cvpr-2023" rel="alternate" type="text/html" title="valeo.ai at CVPR 2023"/><published>2023-06-14T00:00:00+00:00</published><updated>2023-06-14T00:00:00+00:00</updated><id>https://valeoai.github.io//posts/valeoai-at-cvpr-2023</id><content type="html" xml:base="https://valeoai.github.io//posts/cvpr-2023"><![CDATA[<p>The <a href="https://cvpr2023.thecvf.com/">IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)</a> is a key event for researchers and engineers working on computer vision and machine learning. At the 2023 edition the <a href="../">valeo.ai</a> team will present six <a href="../publications/">papers</a> in the main conference, one workshop <a href="https://vision4allseason.net/">keynote</a> and organize a <a href="https://osimeoni.github.io/object-localization-for-free/">tutorial</a>. The team will be at CVPR to present these works and will be happy to discuss more about these projects and ideas, and share our exciting ongoing research. We outline four of our team papers below.</p> <h2 id="octet-object-aware-counterfactual-explanations">OCTET: Object-aware Counterfactual Explanations</h2> <h4 id="authors-mehdi-zemni-mickaël-chen-éloi-zablocki-hédi-ben-younes-patrick-pérez-matthieu-cord">Authors: Mehdi Zemni, <a href="https://scholar.google.com/citations?user=QnRpMJAAAAAJ&amp;hl=fr&amp;oi=sra">Mickaël Chen</a>, <a href="https://scholar.google.com/citations?user=dOkbUmEAAAAJ&amp;hl=fr">Éloi Zablocki</a>, <a href="https://scholar.google.com/citations?hl=fr&amp;user=IFLcfvUAAAAJ">Hédi Ben-Younes</a>, <a href="https://ptrckprz.github.io/">Patrick Pérez</a>, <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2211.12380">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/octet">Code</a>] &nbsp;&nbsp; [<a href="https://www.youtube.com/watch?v=Xfq0uRcw9jQ">Video</a>] &nbsp;&nbsp; [<a href="../publications/octet/">Project page</a>]</h4> <p>Nowadays, deep vision models are being widely deployed in safety-critical applications, e.g., autonomous driving, and explainability of such models is becoming a pressing concern. Among explanation methods, counterfactual explanations aim to find minimal and interpretable changes to the input image that would also change the output of the model to be explained. Such explanations point end-users at the main factors that impact the decision of the model. However, previous methods struggle to explain decision models trained on images with many objects, e.g., urban scenes, which are more difficult to work with but also arguably more critical to explain. In this work, we propose to tackle this issue with an object-centric framework for counterfactual explanation generation. Our method, inspired by recent generative modeling works, encodes the query image into a latent space that is structured in a way to ease object-level manipulations. Doing so, it provides the end-user with control over which search directions (e.g., spatial displacement of objects, style modification, etc.) are to be explored during the counterfactual generation. We conduct a set of experiments on counterfactual explanation benchmarks for driving scenes, and we show that our method can be adapted beyond classification, e.g., to explain semantic segmentation models. To complete our analysis, we design and run a user study that measures the usefulness of counterfactual explanations in understanding a decision model.</p> <p><img src="/assets/img/posts/2023_cvpr/octet.png" alt="octet_overview" height="80%" width="80%"/></p> <div class="caption"><b>Counterfactual explanations generated by OCTET.</b> Given a classifier that predicts whether or not it is possible to go left, and a query image (top left), OCTET produces a counterfactual explanation where the most influential features that led to the decision are changed (top right). On the bottom row, we show that OCTET can also operate under different settings that result in different focused explanations. We report the prediction made by the decision model at the top left of each image. </div> <hr/> <h2 id="also-automotive-lidar-self-supervision-by-occupancy-estimation">ALSO: Automotive Lidar Self-supervision by Occupancy estimation</h2> <h4 id="authors-alexandre-boulch-corentin-sautier-björn-michele-gilles-puy-renaud-marlet">Authors: <a href="https://www.boulch.eu/">Alexandre Boulch</a>, <a href="https://scholar.google.com/citations?user=xYDkHEsAAAAJ">Corentin Sautier</a>, <a href="https://scholar.google.com/citations?user=xQcKnXkAAAAJ&amp;hl=en">Björn Michele</a>, <a href="https://sites.google.com/site/puygilles/home">Gilles Puy</a>, <a href="http://imagine.enpc.fr/~marletr/">Renaud Marlet</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2212.05867">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/ALSO">Code</a>] &nbsp;&nbsp; [<a href="https://www.youtube.com/watch?v=GGIBKlMvphw">Video</a>] &nbsp;&nbsp; [<a href="../publications/also/">Project page</a>]</h4> <p>We propose a new self-supervised method for pre-training the backbone of deep perception models operating on point clouds. The core idea is to train the model on a pretext task which is the reconstruction of the surface on which the 3D points are sampled, and to use the underlying latent vectors as input to the perception head. The intuition is that if the network is able to reconstruct the scene surface, given only sparse input points, then it probably also captures some fragments of semantic information that can be used to boost an actual perception task. This principle has a very simple formulation, which makes it both easy to implement and widely applicable to a large range of 3D sensors and deep networks performing semantic segmentation or object detection. In fact, it supports a single-stream pipeline, as opposed to most contrastive learning approaches, allowing training on limited resources. We conducted extensive experiments on various autonomous driving datasets, involving very different kinds of lidars, for both semantic segmentation and object detection. The results show the effectiveness of our method to learn useful representations without any annotation, compared to existing approaches.</p> <p><img src="/assets/img/posts/2023_cvpr/also.png" alt="also_overview" height="100%" width="100%"/></p> <div class="caption"><b>ALSO overview.</b> The backbone to pre-train produces latent vectors for each input point. At pre-training time, the latent vector are fed into an volumetric occupancy head that classifies query points as full or empty. At semantic training or test time, the same latent vectors are fed into a semantic head, e.g., for semantic segmentation or object detection. </div> <hr/> <h2 id="unsupervised-object-localization-observing-the-background-to-discover-objects">Unsupervised Object Localization: Observing the Background to Discover Objects</h2> <h4 id="authors-oriane-siméoni-chloé-sekkat-gilles-puy-antonin-vobecky-éloi-zablocki-patrick-pérez">Authors: <a href="https://osimeoni.github.io/">Oriane Siméoni</a>, <a href="https://github.com/chloeskt">Chloé Sekkat</a>, <a href="https://sites.google.com/site/puygilles/home">Gilles Puy</a>, <a href="https://vobecant.github.io/">Antonin Vobecky</a>, <a href="https://scholar.google.com/citations?user=dOkbUmEAAAAJ&amp;hl=fr">Éloi Zablocki</a>, <a href="https://ptrckprz.github.io/">Patrick Pérez</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2212.07834">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/FOUND">Code</a>] &nbsp;&nbsp; [<a href="https://youtu.be/jfYQfFcrJBE">Video</a>] &nbsp;&nbsp; [<a href="../publications/found">Project page</a>]</h4> <p>Recent advances in self-supervised visual representation learning have paved the way for unsupervised methods tackling tasks such as object discovery and instance segmentation. However, discovering objects in an image with no supervision is a very hard task; what are the desired objects, when to separate them into parts, how many are there, and of what classes? The answers to these questions depend on the tasks and datasets of evaluation. In this work, we take a different approach and propose to look for the background instead. This way, the salient objects emerge as a by-product without any strong assumption on what an object should be. We propose FOUND, a simple model made of a single conv1 × 1 initialized with coarse background masks extracted from self-supervised patch-based representations. After fast training and refining these seed masks, the model reaches state-of-the-art results on unsupervised saliency detection and object discovery benchmarks. Moreover, we show that our approach yields good results in the unsupervised semantic segmentation retrieval task.</p> <p><img src="/assets/img/posts/2023_cvpr/found.png" alt="found_overview" height="65%" width="65%"/></p> <div class="caption"><b>Overview of FOUND. </b>In the first stage (green upperpart), a background mask is discovered by mining a seed patch through a reweighting of the self-attention maps of a frozen DINO self-supervised features. This seed is then used to find similar patches likely belonging to the background. In the second stage (blue lower part), we train a lightweight 1 × 1 convolutional layer that produces refined masks from the self-supervised features. It is trained in a self-supervised fashion to predict both smoothed inverse coarse masks of the first step, and smoothed binarized version of its own output. Blue arrows denote where the gradients flow (in the reverse direction).</div> <hr/> <h2 id="rangevit-towards-vision-transformers-for-3d-semantic-segmentation-in-autonomous-driving">RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving</h2> <h4 id="authors-angelika-ando-spyros-gidaris-andrei-bursuc-gilles-puy-alexandre-boulch-renaud-marlet">Authors: Angelika Ando, <a href="https://scholar.google.fr/citations?user=7atfg7EAAAAJ&amp;hl=en">Spyros Gidaris</a>, <a href="https://abursuc.github.io/">Andrei Bursuc</a>, <a href="https://sites.google.com/site/puygilles/home">Gilles Puy</a>, <a href="https://www.boulch.eu/">Alexandre Boulch</a>, <a href="http://imagine.enpc.fr/~marletr/">Renaud Marlet</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2301.10222">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/rangevit">Code</a>] &nbsp;&nbsp; [<a href="https://www.youtube.com/watch?v=urd2ZIJ70WY">Video</a>] &nbsp;&nbsp; [<a href="../publications/rangevit/">Project page</a>]</h4> <p>Semantic segmentation of LiDAR point clouds permits vehicles to perceive their surrounding 3D environment independently of the lighting condition, providing useful information to build safe and reliable vehicles. A common approach to segment large scale LiDAR point clouds is to project the points on a 2D surface and then to use regular CNNs, originally designed for images, to process the projected point clouds. These projection-based methods usually benefit from fast computations and, when combined with techniques which use other point cloud representations, achieve state-of-the-art results.</p> <p>Today, projection-based methods leverage 2D CNNs but recent advances in computer vision show that vision transformers (ViTs) have achieved state-of-the-art results in many image-based benchmarks. Despite the absence of almost any domain-specific inductive bias apart from the image tokenization process, ViTs have a strong representation learning capacity and achieve excellent results on various image perception tasks, such as image classification, object detection or semantic segmentation. Inspired by this success of ViTs for image understanding, in this work, we show that projection-based methods for 3D semantic segmentation can benefit from these latest improvements on ViTs when combined with three key ingredients, all described in our <a href="https://arxiv.org/abs/2301.10222">paper</a>.</p> <p><img src="/assets/img/posts/2023_cvpr/rangevit-teaser.png" alt="rangevit_teaser" height="75%" width="75%"/></p> <div class="caption"><b>Exploiting vision transformer (ViT) architectures and weights for LiDAR point cloud semantic segmentation.</b> We leverage the flexibility of transformer-based architectures to re-purpose them with minimal changes for processing sparse point clouds in autonomous driving tasks. The common ViT backbone across modalities allows to effectively transfer weights pre-trained on large image repositories towards improving point cloud segmentation performance with fine-tuning. </div> <p>We answer positively but only after combining them with three key ingredients: (a) ViTs are notoriously hard to train and require a lot of training data to learn powerful representations. By preserving the same backbone architecture as for RGB images, we can exploit the knowledge from long training on large image collections that are much cheaper to acquire and annotate than point clouds. We reach our best results with pre-trained ViTs on large image datasets. (b) We compensate for ViTs’ lack of inductive bias by substituting a tailored non-linear convolutional stem for the classical linear embedding layer. (c) We refine pixel-wise predictions with a convolutional decoder and a skip connection from the convolutional stem to combine low-level but fine-grained features of the convolutional stem with the high-level but coarse predictions of the ViT encoder. With these ingredients, we show that our method, called RangeViT, outperforms prior projection-based methods on nuScenes and SemanticKITTI.</p> <p><img src="/assets/img/posts/2023_cvpr/rangevit-overview.png" alt="rangevit_overview" height="100%" width="100%"/></p> <div class="caption"><b> Overview of RangeViT architecture.</b> First, the point cloud is projected in a 2D space with range projection. Then, the produced range image is processed by the convolutional stem, the ViT encoder and the decoder to obtain a 2D feature map. It is then processed by a 3D refiner layer for 3D point-wise predictions. Note that there is a single skip connection between the convolutional stem and the decoder. </div> <p>In summary, our work offers the following contributions:</p> <ul> <li>Exploiting the powerful representation learning capacity of vision transformers for LiDAR semantic segmentation.</li> <li>Unifying the network architectures for processing LiDAR point clouds and images, enabling advancements in one domain to benefit both.</li> <li>Demonstrating the utilization of pre-trained ViTs on large-scale natural image datasets for LiDAR point cloud segmentation.</li> </ul> <p>We believe that this finding is highly intriguing. The RangeViT approach can leverage off-the-shelf pre-trained ViT models, enabling direct benefits from ongoing and future advances in training ViT models with natural RGB images - a rapidly growing research field.</p>]]></content><author><name></name></author><category term="3d perception"/><category term="multi-sensor"/><category term="limited supervision"/><category term="reliability"/><summary type="html"><![CDATA[Alexandre Boulch, Oriane Siméoni, Gilles Puy, Éloi Zablocki, Spyros Gidaris, Andrei Bursuc]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io//assets/img/posts/2023_cvpr/cvpr_banner.svg"/><media:content medium="image" url="https://valeoai.github.io//assets/img/posts/2023_cvpr/cvpr_banner.svg" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">valeo.ai at CVPR 2022</title><link href="https://valeoai.github.io//posts/cvpr-2022" rel="alternate" type="text/html" title="valeo.ai at CVPR 2022"/><published>2022-06-14T00:00:00+00:00</published><updated>2022-06-14T00:00:00+00:00</updated><id>https://valeoai.github.io//posts/valeoai-at-cvpr-2022</id><content type="html" xml:base="https://valeoai.github.io//posts/cvpr-2022"><![CDATA[<p>The <a href="https://cvpr2022.thecvf.com/">IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)</a> is a major event for researchers and engineers working on computer vision and machine learning. At the 2022 edition the <a href="../">valeo.ai</a> team will present four <a href="../publications/">papers</a> in the main conference, three <a href="../publications/">papers</a> in workshops and one workshop <a href="https://vision4allseason.net/">keynote</a>. The team will be at CVPR to present these works and will be happy to discuss more about these projects and ideas, and share our exciting ongoing research.</p> <h2 id="image-to-lidar-self-supervised-distillation-for-autonomous-driving-data">Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data</h2> <h4 id="authors-corentin-sautier-gilles-puy--spyros-gidaris--alexandre-boulch-andrei-bursuc-renaud-marlet">Authors: Corentin Sautier, <a href="https://sites.google.com/site/puygilles/home">Gilles Puy</a>, <a href="https://scholar.google.fr/citations?user=7atfg7EAAAAJ&amp;hl=en">Spyros Gidaris</a>, <a href="https://www.boulch.eu/">Alexandre Boulch</a>, <a href="https://abursuc.github.io/">Andrei Bursuc</a>, <a href="http://imagine.enpc.fr/~marletr/">Renaud Marlet</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2203.16258">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/SLidR">Code</a>] &nbsp;&nbsp; [<a href="../publications/slidr/">Project page</a>]</h4> <p>Self-driving vehicles require object detection or segmentation to safely maneuver in their environment. Such safety-critical tasks are usually performed by neural networks demanding huge Lidar datasets with high quality annotations, and no domain shift between training and testing conditions. However, annotating 3D Lidar data for these tasks is tedious and costly. In <a href="https://arxiv.org/abs/2203.16258">Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data</a>, we propose a self-supervised pre-training method for 3D perception models that is tailored to autonomous driving data and that does not require any annotation. Specifically, we leverage the availability of synchronized and calibrated image and Lidar data in autonomous driving setups for distilling self-supervised pre-trained image representations into 3D models, using neither point cloud nor image annotations.</p> <p><img src="/assets/img/posts/2022_cvpr/SLidR_overview_2.png" alt="slidr_overview" height="100%" width="100%"/></p> <div class="caption"><b>Synchronized Lidar and camera frames are encoded through two modality-specific features extractors.</b> The camera backbone has pre-trained weights obtained with no annotations (e.g., with MoCo v2 <a class="citation" href="#chen2020improved">(Chen et al., 2020)</a>). Features are pooled at a pseudo-object level using image superpixels, and contrasted between both modalities</div> <p>A key ingredient of our method is the use of superpixels which are used to pool 3D point features and 2D pixel features in visually similar regions. We then train a 3D network on the self-supervised task of matching these pooled 3D-point features with the corresponding pooled image pixel features. Extensive experiments on autonomous driving datasets demonstrate the ability of our image-to-Lidar distillation strategy to produce 3D representations that transfer well to semantic segmentation and object detection tasks.</p> <p><img src="/assets/img/posts/2022_cvpr/SLidR_results.jpg" alt="slidr_results" height="100%" width="100%"/></p> <div class="caption">The similarity between a query point's features (in red) and all other Lidar points is shown, to assert the quality of the learned representation. Color scale goes from purple (low similarity) to yellow (high similarity). </div> <p>With our pre-training, a Lidar network can learn features that are mostly consistent within an object class. This pre-training greatly improves data annotation efficiency, both in semantic segmentation and object detection, and is even applicable in cross-dataset setups.</p> <hr/> <h2 id="raw-high-definition-radar-for-multi-task-learning">Raw High-Definition Radar for Multi-Task Learning</h2> <h4 id="authors--julien-rebut-arthur-ouaknine-waqas-walik-patrick-pérez">Authors: <a href="https://scholar.google.com/citations?user=BJcQNcoAAAAJ">Julien Rebut</a>, <a href="https://arthurouaknine.github.io/">Arthur Ouaknine</a>, Waqas Walik, <a href="https://ptrckprz.github.io/">Patrick Pérez</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2112.10646">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/RADIal">Code</a>] &nbsp;&nbsp; [<a href="../publications/radial/">Project page</a>]</h4> <p>With their robustness to adverse weather conditions and their ability to measure speeds, radar sensors have been part of the automotive landscape for more than two decades. Recent progress toward High Definition (HD) Imaging radars has driven the angular resolution below the degree, thus approaching laser scanning performance. However, the amount of data a HD radar delivers and the computational cost to estimate the angular positions remain a challenge. In this paper, we propose a novel HD radar sensing model, FFT-RadNet, that eliminates the overhead of computing the range-azimuth-Doppler 3D tensor, learning instead to recover angles from a range-Doppler spectrum. This architecture can be leveraged for various perception tasks with raw HD radar signals. In particular we show how to train FFT-RadNet both to detect vehicles and to segment free driving space. On both tasks, it competes with the most recent radar-based models while requiring less compute and memory.</p> <p><img src="/assets/img/posts/2022_cvpr/radial_overview.png" alt="" height="100%" width="100%"/></p> <div class="caption">Overview of FFT-RadNet for vehicle detection and drivable space segmentation in raw HD radar signal.</div> <p>Also, and importantly, we collected and annotated 2-hour worth of raw data from synchronized automotive-grade sensors (camera, laser, HD radar) in various environments (city street, highway, countryside road). This unique dataset, nick-named RADIal for “Radar, Lidar et al.”, is <a href="../publications/radial/">publicly available</a>.</p> <p><img src="/assets/img/posts/2022_cvpr/radial_teaser.jpg" alt="" height="100%" width="100%"/></p> <div class="caption"><b>Scene sample form RADIal dataset</b> with (a) camera image, (b) radar power spectrum, (c) free-space in bird-eye view, (d) Range-azimuth map in Cartesian coordinates, and (e) GPS trace (red) and odometry trajectory (green); laser (resp. radar) points are in red (resp. indigo), annotated vehicle bounding boxes in orange and annotated drivable space in green.</div> <hr/> <h2 id="poco-point-convolution-for-surface-reconstruction">POCO: Point convolution for surface reconstruction</h2> <h4 id="authors-alexandre-boulch-renaud-marlet">Authors: <a href="https://boulch.eu/">Alexandre Boulch</a>, <a href="http://imagine.enpc.fr/~marletr/">Renaud Marlet</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2201.01831">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/POCO">Code</a>] &nbsp;&nbsp; [<a href="../publications/poco/">Project page</a>]</h4> <p>Implicit neural networks have been successfully used for surface reconstruction from point clouds. However, many of them face scalability issues as they encode the isosurface function of a whole object or scene into a single latent vector. To overcome this limitation, a few approaches infer latent vectors on a coarse regular 3D grid or on 3D patches, and interpolate them to answer occupancy queries. In doing so, they lose the direct connection with the input points sampled on the surface of objects, and they attach information uniformly in space rather than where it matters the most, i.e., near the surface. Besides, relying on fixed patch sizes may require discretization tuning.</p> <p>In POCO, we propose to use point cloud convolution and compute a latent vector at each input point. We then perform a learning-based interpolation on nearest neighbors using inferred weights. On the one hand, using a convolutional backbone allows the aggregation of global information about the shape needed to correctly orientate the surface (decide which side of the surface is inside or outside). On the other hand, surface location is inferred via a local attention-based approach which enables accurate surface positioning.</p> <p><img src="/assets/img/posts/2022_cvpr/poco_teaser.png" alt="" height="100%" width="100%"/></p> <div class="caption"><b>POCO overview.</b> Top row: the decoding mechanism takes as input local latent vectors and local coordinates which are lifted with a point-wise MLP. The resulting representations are weighted with an attention mechanism in order to take the occupancy decision. Bottom row: reconstruction examples with POCO, scene reconstruction with a model trained on objects (left), object reconstruction with noisy point cloud (middle) and out of domain object reconstruction (right). </div> <p>We show that our approach, while being very simple to set up, reaches the state of the art on several reconstruction-from-point-cloud benchmarks. It underlines the importance of reasoning about the surface location at a local scale, close to the input points. POCO also shows good generalization properties including the possibility of learning on object datasets while being able to reconstruct complex scenes.</p> <hr/> <h2 id="dytox-transformers-for-continual-learning-with-dynamic-token-expansion">DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion</h2> <h4 id="authors-arthur-douillard--alexandre-ramé--guillaume-couairon-matthieu-cord">Authors: <a href="https://arthurdouillard.com/">Arthur Douillard</a>, <a href="https://alexrame.github.io/">Alexandre Ramé</a>, Guillaume Couairon, <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2111.11326">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/arthurdouillard/dytox">Code</a>] </h4> <p>Deep network architectures struggle to continually learn new tasks without forgetting the previous tasks. In this paper, we propose a transformer architecture based on a dedicated encoder/decoder framework. Critically, the encoder and decoder are shared among all tasks. Through a dynamic expansion of special tokens, we specialize each forward of our decoder network on a task distribution. Our strategy scales to a large number of tasks while having negligible memory and time overheads due to strict control of the parameters expansion. Moreover, this efficient strategy does not need any hyperparameter tuning to control the network’s expansion. Our model reaches excellent results on CIFAR100 and state-of-the-art performances on the large-scale ImageNet100 and ImageNet1000 while having fewer parameters than concurrent dynamic frameworks.</p> <p><img src="/assets/img/posts/2022_cvpr/dytox.png" alt="" height="85%" width="85%"/></p> <div class="caption"> DyTox transformer model.</div> <hr/> <h2 id="flexit-towards-flexible-semantic-image-translation">FlexIT: Towards Flexible Semantic Image Translation</h2> <h4 id="authors-guillaume-couairon-asya-grechka-jakob-verbeek-holger-schwenk-matthieu-cord">Authors: Guillaume Couairon, Asya Grechka, <a href="https://lear.inrialpes.fr/people/verbeek/">Jakob Verbeek</a>, <a href="https://scholar.google.fr/citations?user=Ysjk8kkAAAAJ&amp;hl=en">Holger Schwenk</a>, <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2203.04705">Paper</a>] </h4> <p>Deep generative models, like GANs, have considerably improved the state of the art in image synthesis, and are able to generate near photo-realistic images in structured domains such as human faces. We propose FlexIT, a novel method which can take any input image and a user-defined text instruction for editing. Our method achieves flexible and natural editing, pushing the limits of semantic image translation. First, FlexIT combines the input image and text into a single target point in the CLIP multimodal embedding space. Via the latent space of an auto-encoder, we iteratively transform the input image toward the target point, ensuring coherence and quality with a variety of novel regularization terms. We propose an evaluation protocol for semantic image translation, and thoroughly evaluate our method on ImageNet.</p> <p><img src="/assets/img/posts/2022_cvpr/flexit.png" alt="" height="95%" width="95%"/></p> <div class="caption"> <b>FlexIT transformation examples.</b> From top to bottom: input image, transformed image, and text query.</div> <hr/> <h2 id="raising-context-awareness-in-motion-forecasting">Raising context awareness in motion forecasting</h2> <p class="page-description"><a href="https://cvpr2022.wad.vision/">CVPR 2022 Workshop on Autonomous Driving</a></p> <h4 id="authors-hédi-ben-younes-éloi-zablocki-mickaël-chen-patrick-pérez-matthieu-cord">Authors: <a href="https://scholar.google.com/citations?hl=fr&amp;user=IFLcfvUAAAAJ">Hédi Ben-Younes</a>, <a href="https://scholar.google.com/citations?user=dOkbUmEAAAAJ&amp;hl=fr">Éloi Zablocki</a>, <a href="https://scholar.google.com/citations?user=QnRpMJAAAAAJ&amp;hl=fr&amp;oi=sra">Mickaël Chen</a>, <a href="https://ptrckprz.github.io/">Patrick Pérez</a>, <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2109.08048">Paper</a>]</h4> <p><img src="/assets/img/posts/2022_cvpr/cab.png" alt="cab_overview" height="50%" width="50%"/></p> <div class="caption"><b>Overview of CAB.</b> CAB employs a CVAE backbone which produces distributions over the latent variable and the future trajectory. During training, a blind input is forwarded into the CVAE and the resulting distribution over the latent variable is used to encourage the prediction of the model to be different from the context-agnostic distribution, thanks to the CAB-KL loss.</div> <p>Learning-based trajectory prediction models have encountered great success, with the promise of leveraging contextual information in addition to motion history. Yet, we find that state-of-the-art forecasting methods tend to overly rely on the agent’s current dynamics, failing to exploit the semantic contextual cues provided at its input. To alleviate this issue, we introduce CAB, a motion forecasting model equipped with a training procedure designed to promote the use of semantic contextual information. We also introduce two novel metrics, dispersion and convergence-to-range, to measure the temporal consistency of successive forecasts, which we found missing in standard metrics. Our method is evaluated on the widely adopted nuScenes Prediction benchmark as well as on a subset of the most difficult examples from this benchmark.</p> <hr/> <h2 id="csg0-continual-urban-scene-generation-with-zero-forgetting">CSG0: Continual Urban Scene Generation with Zero Forgetting</h2> <p class="page-description"><a href="https://sites.google.com/view/clvision2022">CVPR 2022 Workshop on Continual Learning (CLVision)</a></p> <h4 id="authors--himalaya-jain-tuan-hung-vu-patrick-pérez-matthieu-cord">Authors: <a href="https://himalayajain.github.io/">Himalaya Jain</a>, <a href="https://tuanhungvu.github.io/">Tuan-Hung Vu</a>, <a href="https://ptrckprz.github.io/">Patrick Pérez</a>, <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2112.03252">Paper</a>] &nbsp;&nbsp; [<a href="../publications/csg0/">Project page</a>]</h4> <p>With the rapid advances in generative adversarial networks (GANs), the visual quality of synthesized scenes keeps improving, including for complex urban scenes with applications to automated driving. We address in this work a continual scene generation setup in which GANs are trained on a stream of distinct domains; ideally, the learned models should eventually be able to generate new scenes in all seen domains. This setup reflects the real-life scenario where data are continuously acquired in different places at different times. In such a continual setup, we aim for learning with zero forgetting, i.e., with no degradation in synthesis quality over earlier domains due to catastrophic forgetting. To this end, we introduce a novel framework, named CSG0, that not only (i) enables seamless knowledge transfer in continual training but also (ii) guarantees zero forgetting with a small overhead cost.</p> <p><img src="/assets/img/posts/2022_cvpr/csg0_teaser.png" alt="" height="85%" width="85%"/></p> <div class="caption"><b>Overview of CSG0.</b> Our continual setup for urban-scene generation involves a stream of datasets, with GANs trained from one dataset to another. Our framework makes use of the knowledge learned from previous domains and adapts to new ones with a small overhead.</div> <p>To showcase the merit of our framework, we conduct intensive experiments on various continual urban scene setups, covering both synthetic-to-real and real-to-real scenarios. Quantitative evaluations and qualitative visualizations demonstrate the interest of our CSG0 framework, which operates with minimal overhead cost (in terms of architecture size and training). Benefiting from continual learning, CSG0 outperforms the state-of-the-art OASIS model trained on single domains. We also provide experiments with three datasets to emphasize how well our strategy generalizes despite its cost constraints. Under extreme low-data regimes, our approach outperforms the baseline by a large margin.</p> <hr/> <h2 id="multi-head-distillation-for-continual-unsupervised-domain-adaptation-in-semantic-segmentation">Multi-Head Distillation for Continual Unsupervised Domain Adaptation in Semantic Segmentation</h2> <p class="page-description"><a href="https://sites.google.com/view/clvision2022">CVPR 2022 Workshop on Continual Learning (CLVision)</a></p> <h4 id="authors--antoine-saporta-arthur-douillard-tuan-hung-vu-patrick-pérez-matthieu-cord">Authors: <a href="https://scholar.google.com/citations?user=jSwfIU4AAAAJ">Antoine Saporta</a>, <a href="https://arthurdouillard.com/">Arthur Douillard</a>, <a href="https://tuanhungvu.github.io/">Tuan-Hung Vu</a>, <a href="https://ptrckprz.github.io/">Patrick Pérez</a>, <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2204.11667">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/MuHDi">Code</a>] &nbsp;&nbsp; [<a href="../publications/muhdi/">Project page</a>]</h4> <p>This work focuses on a novel framework for learning UDA, continuous UDA, in which models operate on multiple target domains discovered sequentially, without access to previous target domains. We propose MuHDi, for Multi-Head Distillation, a method that solves the catastrophic forgetting problem, inherent in continual learning tasks. MuHDi performs distillation at multiple levels from the previous model as well as an auxiliary target-specialist segmentation head. We report both extensive ablation and experiments on challenging multi-target UDA semantic segmentation benchmarks to validate the proposed learning scheme and architecture.</p> <p><img src="/assets/img/posts/2022_cvpr/muhdi_teaser.png" alt="" height="90%" width="90%"/></p> <div class="caption"><b>Predictions of continual baseline and MuHDi in a Cityscapes scene.</b> The baseline model suffers from catastrophic forgetting when adapting from one domain to another. The proposed MuHDi is more resilient to continual adaptation and preserve predictive accuracy. </div> <h2 id="references">References</h2> <ol class="bibliography"><li> <div class="row"> Xinlei Chen,&nbsp;Haoqi Fan,&nbsp;Ross Girshick,&nbsp;and&nbsp;Kaiming He.&nbsp; Improved baselines with momentum contrastive learning.&nbsp; <em>arXiv preprint arXiv:2003.04297</em>,&nbsp;2020. </div> </li></ol>]]></content><author><name></name></author><category term="domain adaptation"/><category term="3d perception"/><category term="multi-sensor"/><category term="limited supervision"/><summary type="html"><![CDATA[Corentin Sautier, Alexandre Boulch, Patrick Pérez, Éloi Zablocki, Tuan-Hung Vu, Matthieu Cord, Andrei Bursuc]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io//assets/img/posts/2022_cvpr/cvpr_logo.png"/><media:content medium="image" url="https://valeoai.github.io//assets/img/posts/2022_cvpr/cvpr_logo.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">valeo.ai at ICCV 2021</title><link href="https://valeoai.github.io//posts/iccv-2021" rel="alternate" type="text/html" title="valeo.ai at ICCV 2021"/><published>2021-10-08T00:00:00+00:00</published><updated>2021-10-08T00:00:00+00:00</updated><id>https://valeoai.github.io//posts/valeoai-at-iccv-2021</id><content type="html" xml:base="https://valeoai.github.io//posts/iccv-2021"><![CDATA[<p>The <a href="https://iccv2021.thecvf.com/home">International Conference on Computer Vision (ICCV)</a> is a top event for researchers and engineers working on computer vision and machine learning. The <a href="../">valeo.ai</a> team will present six <a href="../publications/">papers</a> in the main conference, four of which are presented below. Join us to find out more about these projects and ideas, meet our team and learn about our exciting ongoing research. See you at ICCV!</p> <h2 id="multi-view-radar-semantic-segmentation">Multi-View Radar Semantic Segmentation</h2> <h4 id="authors-arthur-ouaknine-alasdair-newson-patrick-pérez-florence-tupin-julien-rebut">Authors: <a href="https://arthurouaknine.github.io/">Arthur Ouaknine</a>, <a href="https://sites.google.com/site/alasdairnewson/">Alasdair Newson</a>, <a href="https://ptrckprz.github.io/">Patrick Pérez</a>, <a href="https://perso.telecom-paristech.fr/tupin/">Florence Tupin</a>, <a href="https://scholar.google.com/citations?user=BJcQNcoAAAAJ&amp;hl=fr">Julien Rebut</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2103.16214">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/MVRSS">Code</a>] &nbsp;&nbsp; [<a href="../publications/mvrss/">Project page</a>]</h4> <p><img src="/assets/img/posts/2021_iccv/radar.gif" alt="radar_overview" height="80%" width="80%"/></p> <div class="caption"><b>Example of a scene from the CARRADA dataset <a class="citation" href="#ouaknine2021carrada">(Ouaknine et al., 2021)</a>.</b> From left to right: camera image, range-angle view, range-Doppler view, angle, Doppler view.</div> <p>Understanding the scene around the ego-vehicle is key to assisted and autonomous driving. Nowadays, this is mostly conducted using cameras and laser scanners, despite their reduced performance in adverse weather conditions. Automotive radars are low-cost active sensors that measure properties of surrounding objects, including their relative speed, and have the key advantage of not being impacted by rain, snow or fog and could effectively complement the other perception sensors mounted on the car, e.g., cameras, LIDAR. However, they are seldom used for scene understanding due to the size and complexity of radar raw data and the lack of annotated datasets. Fortunately, recent open-sourced datasets have opened up research on classification, object detection and semantic segmentation with raw radar signals using end-to-end trainable models.</p> <p><img src="/assets/img/posts/2021_iccv/radar_semseg.png" alt="mvrss_overview" height="80%" width="80%"/></p> <div class="caption">Sequences of raw radar tensors are aggregated and used as input for our multi-view architecture to segment semantically range-angle and range-Doppler views simultaneously. </div> <p>In our paper, <a href="https://arxiv.org/abs/2103.16214">Multi-View Radar Semantic Segmentation</a>, we propose a set of deep neural network architectures to segment simultaneously range-angle and range-Doppler radar representations, providing the location and the radial velocity of the detected objects. Our best model takes a sequence of radar views as input, extracts features using individual branches including ASPP blocks, and recovers the range-angle and range-Doppler view dimensions with two decoding branches. We also propose a combination of loss functions composed of a weighted cross entropy, a soft dice and an additional coherence term. We introduce a coherence loss to impose a spatial consistency between the segmented radar views. Our experiments on the CARRADA dataset <a class="citation" href="#ouaknine2021carrada">(Ouaknine et al., 2021)</a> demonstrate that our best model outperforms competing methods with a large margin while requiring significantly fewer parameters.</p> <hr/> <h2 id="triggering-failures-out-of-distribution-detection-by-learning-from-local-adversarial-attacks-in-semantic-segmentation">Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation</h2> <h4 id="authors-victor-besnier-andrei-bursuc-alexandre-briot-david-picard">Authors: <a href="https://scholar.google.com/citations?user=n_C2h-QAAAAJ">Victor Besnier</a>, <a href="https://abursuc.github.io/">Andrei Bursuc</a>, Alexandre Briot, <a href="https://davidpicard.github.io/">David Picard</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2108.01634">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/obsnet">Code</a>] &nbsp;&nbsp; [<a href="../publications/obsnet/">Project page</a>]</h4> <p><img src="/assets/img/posts/2021_iccv/obsnet_qualitative.png" alt="" height="100%" width="100%"/></p> <div class="caption"><b>Uncertainty map visualization on the BDD-Anomaly dataset.</b> 1st col.: We highlight the ground truth locations of the OOD objects to help visualize them (red bounding box). 2nd col.: Segmentation map of the SegNet. 3rd to 5th col.: Uncertainty Map highlighted in yellow. Our method produces stronger responses on OOD regions compared to other methods, while being as strong on regular error regions, e.g., boundaries. </div> <p>For real-world decision systems such as autonomous vehicles, accuracy is not the only performance requirement and it often comes second to <em>reliability</em>, <em>robustness</em>, and <em>safety concerns</em>, as any failure carries serious consequences. Component modules of such systems frequently rely on powerful Deep Neural Networks (DNNs), that however do not always generalize to objects unseen in the training data. Simple uncertainty estimation techniques, e.g., entropy of softmax predictions, are less effective since modern DNNs are consistently overconfident on both in-domain and out-of-distribution (OOD) data samples. This hinders further the performance of downstream components relying on their predictions. Dealing successfully with the <em>“unknown unknown”</em>, e.g., by launching an alert or failing gracefully, is crucial.</p> <p><img src="/assets/img/posts/2021_iccv/robot_hit.gif" alt="" height="50%" width="50%"/></p> <div class="caption">By making our target model to fail we can learn its behavior when failing and more reliably detect it at test time.</div> <p>In this work we take inspiration from practices in industrial validation, where the performance of a target model is tested in various extreme cases. Instead of simply verifying the performance of the model we learn how this model behaves in face of failures. To this end we propose a new OOD detection architecture called ObsNet and an associated training scheme based on Local Adversarial Attacks (LAA). Finding failure modes in a trained DNN is quite challenging as such models typically achieve high accuracy, i.e., are rarely wrong, and corner-case samples are rather inserted in the training set than used for validation. LAA triggers failure modes in the target model that are a good proxy for failures in face of unknown OOD data. ObsNet achieves reliable detection of failure and OOD objects without compromising on predictive accuracy and computational time.</p> <hr/> <h2 id="multi-target-adversarial-frameworks-for-domain-adaptation-in-semantic-segmentation">Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation</h2> <h4 id="authors--antoine-saporta-tuan-hung-vu-matthieu-cord-patrick-pérez">Authors: <a href="https://scholar.google.com/citations?user=jSwfIU4AAAAJ">Antoine Saporta</a>, <a href="https://tuanhungvu.github.io/">Tuan-Hung Vu</a>, <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a>, <a href="https://ptrckprz.github.io/">Patrick Pérez</a></h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2108.06962">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/MTAF">Code</a>] &nbsp;&nbsp; [<a href="../publications/mtaf/">Project page</a>]</h4> <p><img src="/assets/img/posts/2021_iccv/mtaf_teaser.png" alt="" height="100%" width="100%"/></p> <div class="caption"><b>Single-target unsupervised domain adaptation fails to produce models that perform on multiple target domains.</b> The aim of multi-target unsupervised domain adaptation is to train a model that excels on these multiple target domains.</div> <p>Autonomous vehicles rely on perception models that require a tremendous amount of annotated data to be trained in a supervised fashion. To reduce the reliance on manual annotation which can get extremely expensive when we consider semantic segmentation of urban scenes for instance, domain adaptation is a popular topic that leverages annotated data from a source domain to train a model on a target domain. More specifically, the unsupervised domain adaptation (UDA) setting only relies on unlabeled data from the target domain and aims at bridging the gap between target and source domains. Most UDA approaches tackle the alignment between a single source domain and a single target domain but don’t generalize well to more domains. Yet, real-world perception systems need to be confronted to a variety of scenarios, such as multiple cities or multiple weather conditions, motivating to extend UDA to multi-target settings.</p> <p>In our work, <a href="https://arxiv.org/abs/2108.06962">Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation</a>, we introduce two UDA frameworks to tackle multi-target adaptation: <strong>(i)</strong> multi-discriminator, which extends single target UDA approaches to multiple target domains by explicitly aligning each target domain to its counterparts; <strong>(ii)</strong> multi-target knowledge transfer, which learns a target-agnostic model thanks to a multiple teachers/single student distillation mechanism. We also propose multiple new challenging evaluation benchmarks for multi-target UDA in semantic segmentation based on existing urban scenes datasets.</p> <hr/> <h2 id="pcam-product-of-cross-attention-matrices-for-rigid-registration-of-point-clouds">PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds</h2> <h4 id="authors-anh-quan-cao-gilles-puy-alexandre-boulch-renaud-marlet">Authors: <a href="https://anhquancao.github.io">Anh-Quan Cao</a>, <a href="https://sites.google.com/site/puygilles/home">Gilles Puy</a>, <a href="https://www.boulch.eu/">Alexandre Boulch</a>, <a href="http://imagine.enpc.fr/~marletr/">Renaud Marlet</a></h4> <h4 align="center"> [<a href="http://arxiv.org/abs/2110.01269">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/PCAM">Code</a>] &nbsp;&nbsp; [<a href="../publications/pcam/">Project page</a>]</h4> <p><img src="/assets/img/posts/2021_iccv/pcam_overview.png" alt="" height="100%" width="100%"/></p> <p>Point cloud registration has many applications in various domains such as autonomous driving, motion and pose estimation, 3D reconstruction, simultaneous localisation and mapping (SLAM), and augmented reality. The most famous method to solve this task is ICP, but is mostly suited for small transformations. Several improvements have been made and the most recent techniques leverage deep learning.</p> <p>The typical pipeline for point cloud registration is <strong>(a)</strong> point matching followed by <strong>(b)</strong> point-pairs filtering to remove incorrect matches in, e.g., non-overlapping regions. One natural way to improve this pipeline is to use deep learning in step <strong>(a)</strong> to obtain point features of high quality and get pairs of matching points with a nearest neighbors search in this learned feature space. Then, one can typically rely on a classical RANSAC-based method in step <strong>(b)</strong>. Another category of methods exploits deep learning in step <strong>(a)</strong> and step <strong>(b)</strong>, as proposed by, e.g., DCP, PRNet, DGR. PCAM belongs to this second category where a first network outputs pairs of matching points and a second network filters incorrect pairs.</p> <p>We construct PCAM by observing that one needs two types of information to correctly match points between two point clouds. First, one needs local fine geometric information to precisely select the best corresponding point. Second, one also needs high-level contextual information to differentiate between points with similar local geometry but from different parts of the scene. Therefore, we compute point correspondences at every layer of our deep network via cross-attention matrices, and combine these matrices via a pointwise multiplication. This simple yet very effective solution naturally ensures that both low-level geometric and high-level context information are exploited when matching points. It also permits to remove spurious matches found only at one scale. Furthermore, these cross-attention matrices are also exploited to exchange information between the point clouds at each layer, allowing the network to use context information to find the best matching point within the overlapping regions.</p> <h2 id="references">References</h2> <ol class="bibliography"><li> <div class="row"> Arthur Ouaknine,&nbsp;Alasdair Newson,&nbsp;Julien Rebut,&nbsp;Florence Tupin,&nbsp;and&nbsp;Patrick Pérez.&nbsp; CARRADA Dataset: Camera and Automotive Radar with Range- Angle- Doppler Annotations.&nbsp; <em>In 2020 25th International Conference on Pattern Recognition (ICPR)</em>,&nbsp;2021. </div> </li></ol>]]></content><author><name></name></author><category term="domain adaptation"/><category term="3d perception"/><category term="reliability"/><category term="multi-sensor"/><category term="limited supervision"/><summary type="html"><![CDATA[Andrei Bursuc, Gilles Puy, Arthur Ouaknine, Antoine Saporta]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io//assets/img/posts/2021_iccv/iccv_logo.png"/><media:content medium="image" url="https://valeoai.github.io//assets/img/posts/2021_iccv/iccv_logo.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">How can we make driving systems explainable?</title><link href="https://valeoai.github.io//posts/explainable-driving" rel="alternate" type="text/html" title="How can we make driving systems explainable?"/><published>2021-02-18T00:00:00+00:00</published><updated>2021-02-18T00:00:00+00:00</updated><id>https://valeoai.github.io//posts/explainable-driving</id><content type="html" xml:base="https://valeoai.github.io//posts/explainable-driving"><![CDATA[<p><em>This post is an introduction to our survey on the explainability of vision-based driving systems, which can be found on arXiv <a href="https://arxiv.org/abs/2101.05307">here</a>.</em></p> <p>Research on autonomous vehicles is blooming thanks to recent advances in deep learning and computer vision, as well as the development of autonomous driving datasets and simulators. The number of academic publications on this subject is rising in most machine learning, computer vision, robotics and transportation conferences, and journals. On the industry side, several suppliers are already producing cars equipped with advanced computer vision technologies for automatic lane following, assisted parking, or collision detection among other things. Meanwhile, constructors are working on and designing prototypes with level 4 and 5 autonomy.</p> <p>In the 2010s, we observe an interest in approaches aiming to <em>train</em> driving systems, usually in the form of neural networks, either by leveraging large quantities of expert recordings or through simulation. In both cases, these systems learn a highly complex transformation that operates over input sensor data and produces end-commands (steering angle, throttle). While these neural driving models overcome some of the limitations of the traditional modular pipeline stack, they are sometimes described as <em>black-boxes</em> for their critical lack of transparency and interpretability. Thus, being able to explain the behavior of neural driving models is of paramount importance for their deployment and social acceptance.</p> <h3 id="explainability">Explainability?</h3> <p>Many terms are related to the concept of explainability and several definitions have been proposed for each of these terms. The boundaries between concepts are fuzzy and constantly evolving. In human-machine interactions, explainability is defined as the ability for the human user to understand the agent’s logic <a class="citation" href="#RosenfeldR19">(Rosenfeld &amp; Richardson, 2019)</a>. The explanation is based on how the human user understands the connections between inputs and outputs of the model. According to <a class="citation" href="#doshi2017accountability">(Doshi-Velez &amp; Kortz, 2017)</a>, an explanation is a human-interpretable description of the process by which a decision-maker took a particular set of inputs and reached a particular conclusion. They state that in practice, an explanation should answer at least one of the three following questions:</p> <ul> <li><em>What were the main factors in the decision?</em></li> <li><em>Would changing a certain factor have changed the decision?</em></li> <li><em>Why did two similar-looking cases get different decisions, or vice versa?</em></li> </ul> <p>The term <em>explainability</em> often co-occurs with the concept of <em>interpretability</em>. Some recent work of <a class="citation" href="#beaudouin2020identifying">(Beaudouin et al., 2020)</a> simply advocate that explainability and interpretability are synonyms. However, <a class="citation" href="#GilpinBYBSK18">(Gilpin et al., 2018)</a> provide a nuance between these terms that we find interesting. According to them, interpretability designates to which extent an explanation is understandable by a human. They state that an explanation should be designed and assessed in a trade-off between its interpretability and its completeness, which measures how accurate the explanation is as it describes the inner workings of the system. The whole challenge in explaining neural networks is to provide explanations that are both interpretable and complete.</p> <p>Interestingly, depending on who is the explanation geared towards, it is expected to have varying nature, form and should convey different types of information.</p> <ul> <li><strong>End-users</strong> and citizens need to trust the autonomous system and to be reassured. They put their life in the hands of the driving system and thus need to gain trust in it. It appears that user trust is heavily impacted by the system transparency <a class="citation" href="#trusthci20">(Zhang et al., 2020)</a>: providing information that helps the user understand how the system functions foster his or her trust in the system. Interestingly, research on human-computer interactions argues that an explanation should be provided before the vehicle takes an action, in a formulation which is concise and direct.</li> <li><strong>Designers</strong> of self-driving models need to understand their limitations to validate them and improve future versions. The concept of Operational Design Domain (ODD) is often used by carmakers to designate the conditions under which the car is expected to behave safely. Thus, whenever a machine learning model is built to address the task of driving, it is crucial to know and understand its failure modes, and to verify that these situations do not overlap with the ODD. A common practice is to stratify the evaluation into situations, as is done by the European New Car Assessment Program (Euro NCAP) to test and assess assisted driving functionalities in new vehicles. But even if these in-depth performance analyses are helpful to improve the model’s performance, it is not possible to exhaustively list and evaluate every situation the model may possibly encounter. As a fallback solution, explainability can help delving deeper into the inner workings of the model and to understand why it makes these errors and correct the model/training data accordingly.</li> <li><strong>Legal and regulatory bodies</strong> are interested in explanations for <em>liability</em> and <em>accountability</em> purposes, especially when a self-driving system is involved in a car accident. Notably, explanations generated for legal or regulatory institutions are likely to be different from those addressed to the end-user, as all aspects of the decision process could be required to identify the reasons for a malfunction. </li> </ul> <h3 id="driving-system">Driving system?</h3> <p>The history of autonomous driving systems started in the late ’80s and early ’90s with the European Eureka project called Prometheus. This has later been followed by <a href="https://www.youtube.com/watch?v=7a6GrKqOxeU">driving challenges</a> proposed by the Defense Advanced Research Projects Agency (DARPA). The vast majority of autonomous systems competing in these challenges is characterized by their modularity: several sub-modules are assembled, each completing a very specific task. Broadly speaking, these subtasks deal with sensing the environment, forecasting future events, planning, taking high-level decisions, and controlling the vehicle.</p> <p>As pipeline architectures split the driving task into easier-to-solve problems, they offer somewhat interpretable processing of sensor data through specialized modules (perception, planning, decision, control). However, these approaches have several drawbacks:</p> <ul> <li>First, they rely on human heuristics and manually-chosen intermediate representations, which are not proven to be optimal for the driving task.</li> <li>Second, they lack flexibility to account for real-world uncertainties and to generalize to unplanned scenarios. </li> <li>Finally, they are prone to error propagation between the multiple sub-modules.</li> </ul> <p>To circumvent these issues, and nurtured by the deep learning revolution, researchers put more and more efforts on machine learning-based driving systems, and in particular on deep neural networks which can leverage large quantities of data.</p> <p>We can distinguish four key elements involved in the design of a neural driving system: input sensors, input representations, output type, and learning paradigm</p> <p><img src="/assets/img/posts/explainable_driving/driving_architecture.png" alt="driving_architecture" width="80%"/></p> <div class="caption"><b>Figure 1. Overview of neural network-based autonomous driving systems.</b></div> <ul> <li><strong>Sensors</strong>. They are the hardware interface through which the neural network perceives its environment. They include cameras, radars, LiDARs, GPS, but also sensors about internal vehicle state such as speed or yaw. For a thorough review of driving sensors, we refer the reader to <a class="citation" href="#survey_sensors">(Yurtsever et al., 2020)</a>.</li> <li><strong>Input representation</strong>. Once sensory inputs are acquired by the system, they are processed by computer vision models to build a structured representation, before being passed to the neural driving system. In the <em>mediated perception</em> approach, several perception systems provide their understanding of the world, and their outputs are aggregated to build an input for the driving model. An example of such vision tasks is object detection and semantic segmentation, tracking objects across time, extracting depth information (<em>i.e.</em> knowing the distance that separates the vehicle from each point in the space), recognizing pedestrian intent… Mediated perception contrasts with the <em>direct perception</em> approach, which instead extracts visual affordances from an image. Affordances are scalar indicators that describe the road situation such as curvature, deviation to neighboring lanes, or distances between ego and other vehicles. </li> <li><strong>Outputs</strong>. Ultimately, the goal is to generate vehicle controls. Some approaches, called end-to-<em>end</em>, tackle this problem by training the deep network to directly output the commands. However, in practice most methods instead predict the future trajectory of the autonomous vehicle; they are called end-to-<em>mid</em> methods. The trajectory is then expected to be followed by a low-level controller, such as the proportional–integral–derivative (PID) controller.</li> <li><strong>Learning</strong>. Two families of methods coexist for training self-driving neural models: <em>behavior cloning</em> approaches, which leverage datasets of human driving sessions, and <em>reinforcement learning</em> approaches, which train models through trial-and-error simulation. <ul> <li>Behavior cloning (BC) approaches leverage huge quantities of recorded human driving sessions to learn the input-output driving mapping by imitation. In this setting, the network is trained to mimic the commands applied by the expert driver (end-to-end models), or the future trajectory (end-to-mid models), in a supervised fashion. An initial attempt to behavior cloning of vehicle controls was made by <a class="citation" href="#Pomerleau88">(Pomerleau, 1988)</a>, and continued later in <a class="citation" href="#pilotnet">(Bojarski et al., 2016)</a>.</li> <li>Reinforcement learning (RL) was alternatively explored by researchers to train neural driving systems. This paradigm learns a policy by balancing self-exploration and reinforcement. This training paradigm relies on a simulator (such as CARLA <a class="citation" href="#carla">(Dosovitskiy et al., 2017)</a>).</li> </ul> </li> </ul> <h3 id="the-challenges-of-explainability-of-neural-driving-systems">The challenges of explainability of neural driving systems</h3> <p>Introducing explainability in the design of learning-based self-driving systems is a challenging task. These concerns arise from two aspects:</p> <ul> <li>From a <strong>Deep Learning perspective</strong>, explainability hurdles of self-driving models are shared with most deep learning models, across many application domains. Indeed, decisions of deep systems are intrinsically hard to explain as the functions these systems represent, mapping from inputs to outputs, are not transparent. In particular, although it may be possible for an expert to broadly understand the structure of the model, the parameter values, which have been learned, are yet to be explained. Several factors cause interpretability issues for self-driving machine learning models. First, a finite training dataset cannot exhaustively cover all possible driving situations. It will likely under- and over-represent some specific cases, and questions such as <em>Has the model encountered situations like X?</em> are legitimate. Moreover, datasets contain numerous biases of various nature (omitted variable bias, cause-effect bias, sampling bias), which also gives rise to explainability issues related to fairness. Second, the mapping function represented by the trained model is poorly understood and is considered as a <em>black-box</em>. The model is highly non-linear and does not provide any robustness guarantee as small input changes may dramatically change the output behavior. Explainability issues thus occur regarding the generalizability and robustness aspects: <em>How will the model behave under these new scenarios?</em> Third, the learning phase is not perfectly understood. Among other things, there are no guarantees that the model will settle at a minimum point that generalizes well to new situations. Thus, the model may learn to ground its decisions on spurious correlations during training instead of on the true causes. We aim at finding answers to questions like <em>Which factors caused this decision to be taken?</em></li> </ul> <p><img src="/assets/img/posts/explainable_driving/ml_challenges.png" alt="ml_challenges" width="80%"/></p> <div class="caption"><b>Figure 2. Explainability hurdles and questions for autonomous driving models, as seen from a machine learning point of view.</b></div> <ul> <li>From a <strong>driving perspective</strong>, it has been shown that humans tackle this task by solving many intermediate sub-problems, at different levels of hierarchy <a class="citation" href="#michon1984critical">(Michon, 1984)</a>. In the effort towards building an autonomous driving system, researchers aim at providing the machine with these intermediate capabilities. Thus, explaining the general behavior of an autonomous vehicle inevitably requires understanding how each of these intermediate steps is carried and how it interacts with others. We can categorize these capabilities into three types: <ul> <li><em>Perception</em>: information about the system’s understanding of its local environment. This includes the objects that have been recognized and assigned to a semantic label (persons, cars, urban furniture, driveable area, crosswalks, traffic lights), their localization, properties of their motion (velocity, acceleration), intentions of other agents, <em>etc</em>.;</li> <li><em>Reasoning</em>: information about how the different components of the perceived environment are organized and assembled by the system. This includes global explanations about the rules that are learned by the model, instance-wise explanation showing which objects are relevant in a given scene, traffic pattern recognition, object occlusion reasoning, <em>etc.</em>;</li> <li><em>Decision</em>: information about how the system processes the perceived environment and its associated reasoning to produce a decision. This decision can be a high-level goal stating that the car should turn right, a prediction of the ego vehicle’s trajectory, its low-level relative motion or even the raw controls, <em>etc</em>.</li> </ul> </li> </ul> <p><img src="/assets/img/posts/explainable_driving/driving_challenges.png" alt="driving_challenges" width="80%"/></p> <div class="caption"><b>Figure 3. Explainability hurdles and questions for autonomous driving models, as seen from an autonomous driving point of view.</b></div> <p>While the separation between perception, reasoning, and decision is clear in modular driving systems, some recent end-to-end neural networks such as PilotNet <a class="citation" href="#pilotnet">(Bojarski et al., 2016)</a> blur the lines and perform these simultaneously. Indeed, when an explanation method is developed for a neural driving system, it is often not clear whether it attempts to explain the perception, the reasoning, or the decision step. Considering the nature of neural networks architecture and training, disentangling perception, reasoning, and decision in neural driving systems constitutes a non-trivial challenge.</p> <h3 id="conclusion">Conclusion</h3> <p>As an answer to such problems, many explanation methods have been proposed and are usually organized into two categories: applying <em>post-hoc methods</em> on an already-trained driving model, and directly building driving models which are inherently interpretable <em>by design</em>. In our <a href="https://arxiv.org/abs/2101.05307">survey</a>, we provide details on existing explainability techniques, show how they tackle to the problem of explaining driving models and highlight their limitations. In addition, we detail remaining challenges and open research avenues to increase explainability of self-driving models. We hope our survey will enable increased awareness in this area from researchers and practitioners in the field, as well as from other potentially related fields.</p> <h3 id="references">References</h3> <ol class="bibliography"><li> <div class="row"> Avi Rosenfeld,&nbsp;and&nbsp;Ariella Richardson.&nbsp; Explainability in human-agent systems.&nbsp; <em>Auton. Agents Multi Agent Syst.</em>,&nbsp;2019. </div> </li> <li> <div class="row"> Finale Doshi-Velez,&nbsp;and&nbsp;Mason A Kortz.&nbsp; Accountability of AI Under the Law: The Role of Explanation.&nbsp; <em>CoRR</em>,&nbsp;2017. </div> </li> <li> <div class="row"> Valérie Beaudouin,&nbsp;Isabelle Bloch,&nbsp;David Bounie,&nbsp;Stéphan Clémençon,&nbsp;Florence d’Alché-Buc,&nbsp;James Eagan,&nbsp;Winston Maxwell,&nbsp;Pavlo Mozharovskyi,&nbsp;and&nbsp;Jayneel Parekh.&nbsp; Flexible and Context-Specific AI Explainability: A Multidisciplinary Approach.&nbsp; <em>CoRR</em>,&nbsp;2020. </div> </li> <li> <div class="row"> Leilani H. Gilpin,&nbsp;David Bau,&nbsp;Ben Z. Yuan,&nbsp;Ayesha Bajwa,&nbsp;Michael Specter,&nbsp;and&nbsp;Lalana Kagal.&nbsp; Explaining Explanations: An Overview of Interpretability of Machine Learning.&nbsp; <em>In DSSA</em>,&nbsp;2018. </div> </li> <li> <div class="row"> Qiaoning Zhang,&nbsp;X. Jessie Yang,&nbsp;and&nbsp;Lionel Peter Robert.&nbsp; Expectations and Trust in Automated Vehicles.&nbsp; <em>In CHI</em>,&nbsp;2020. </div> </li> <li> <div class="row"> Jacob Haspiel,&nbsp;Na Du,&nbsp;Jill Meyerson,&nbsp;Lionel P. Robert Jr.,&nbsp;Dawn M. Tilbury,&nbsp;X. Jessie Yang,&nbsp;and&nbsp;Anuj K. Pradhan.&nbsp; Explanations and Expectations: Trust Building in Automated Vehicles.&nbsp; <em>In HRI</em>,&nbsp;2018. </div> </li> <li> <div class="row"> Na Du,&nbsp;Jacob Haspiel,&nbsp;Qiaoning Zhang,&nbsp;Dawn Tilbury,&nbsp;Anuj K Pradhan,&nbsp;X Jessie Yang,&nbsp;and&nbsp;Lionel P Robert Jr.&nbsp; Look who’s talking now: Implications of AV’s explanations on driver’s trust, AV preference, anxiety and mental workload.&nbsp; <em>Transportation research part C: emerging technologies</em>,&nbsp;2019. </div> </li> <li> <div class="row"> Ekim Yurtsever,&nbsp;Jacob Lambert,&nbsp;Alexander Carballo,&nbsp;and&nbsp;Kazuya Takeda.&nbsp; A Survey of Autonomous Driving: Common Practices and Emerging Technologies.&nbsp; <em>IEEE Access</em>,&nbsp;2020. </div> </li> <li> <div class="row"> Chenyi Chen,&nbsp;Ari Seff,&nbsp;Alain L. Kornhauser,&nbsp;and&nbsp;Jianxiong Xiao.&nbsp; DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving.&nbsp; <em>In ICCV</em>,&nbsp;2015. </div> </li> <li> <div class="row"> Marin Toromanoff,&nbsp; Émilie Wirbel,&nbsp;and&nbsp;Fabien Moutarde.&nbsp; End-to-End Model-Free Reinforcement Learning for Urban Driving Using Implicit Affordances.&nbsp; <em>In CVPR</em>,&nbsp;2020. </div> </li> <li> <div class="row"> Dean Pomerleau.&nbsp; ALVINN: An Autonomous Land Vehicle in a Neural Network.&nbsp; <em>In NIPS</em>,&nbsp;1988. </div> </li> <li> <div class="row"> Mariusz Bojarski,&nbsp;Davide Del Testa,&nbsp;Daniel Dworakowski,&nbsp;Bernhard Firner,&nbsp;Beat Flepp,&nbsp;Prasoon Goyal,&nbsp;Lawrence D. Jackel,&nbsp;Mathew Monfort,&nbsp;Urs Muller,&nbsp;Jiakai Zhang,&nbsp;Xin Zhang,&nbsp;Jake Zhao,&nbsp;and&nbsp;Karol Zieba.&nbsp; End to End Learning for Self-Driving Cars.&nbsp; <em>CoRR</em>,&nbsp;2016. </div> </li> <li> <div class="row"> Alexey Dosovitskiy,&nbsp;Germán Ros,&nbsp;Felipe Codevilla,&nbsp;Antonio López,&nbsp;and&nbsp;Vladlen Koltun.&nbsp; CARLA: An Open Urban Driving Simulator.&nbsp; <em>In CoRL</em>,&nbsp;2017. </div> </li> <li> <div class="row"> J.A. Michon.&nbsp; A Critical View of Driver Behavior Models: What Do We Know, what Should We Do?.&nbsp; 1984. </div> </li></ol>]]></content><author><name></name></author><category term="explainability"/><category term="interpretability"/><category term="survey"/><category term="self-driving"/><summary type="html"><![CDATA[Hedi Ben younes*, Eloi Zablocki*, Matthieu Cord and Patrick Pérez]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io//assets/img/posts/explainable_driving/logo_explainable.png"/><media:content medium="image" url="https://valeoai.github.io//assets/img/posts/explainable_driving/logo_explainable.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving</title><link href="https://valeoai.github.io//posts/plop" rel="alternate" type="text/html" title="PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving"/><published>2020-11-26T00:00:00+00:00</published><updated>2020-11-26T00:00:00+00:00</updated><id>https://valeoai.github.io//posts/plop-trajectory-prediction</id><content type="html" xml:base="https://valeoai.github.io//posts/plop"><![CDATA[<p><em>This post describes our <a href="https://drive.google.com/file/d/1--QAL2sR7KMk9R4DwxyfJAT5iGCheFrn/view">recent work</a> on probabilistic trajectory prediction for autonomous driving presented at <a href="https://www.robot-learning.org/home">CORL 2020</a>. PLOP is a trajectory prediction method that intent to control an autonomous vehicle (ego vehicle) in urban environment while considering and predicting the intents of other road users (neighbors). We focus here on predicting multiple feasible future trajectories for both ego vehicle and neighbors through a probabilistic framework and rely on a conditional imitation learning algorithm, conditioned by a navigation command for the ego vehicle (e.g., ``turn right’’). Our model processes only onboard sensor data (camera and lidars) along with detections of past and presents objects relaxing the necessity of an HDMap and is computationally efficient as it can run in real time (25 fps) on an embedded board in the real vehicle. We evaluate our method offline on the publicly available dataset nuScenes <a class="citation" href="#holger2020nuscenes">(Caesar et al., 2020)</a>, achieving state-of-the-art performance, investigate the impact of our architecture choices on online simulated experiments and show preliminary insights for real vehicle control.</em></p> <p><img src="/assets/img/posts/plop/plop.png" alt="plop_teaser" height="60%" width="60%"/></p> <div class="caption"><b>Figure 1. Qualitative example of trajectory predictions on a test sample from nuScenes dataset.</b> The top image show a bird's eye view of PLOP's predictions for the ego and neighbor vehicles (to be compared with the ground truth in green). The bottom row present the input image (left) in which we added object correspondance with the bird's eye view and the auxiliary semantic segmentation of this image (right)</div> <p>Predicting the future positions of other agents of the road, or of the autonomous vehicle itself, is critical for autonomous driving. This trajectory prediction must not only respect the rules of the road, but capture the interactions of the agents over time. It is also important to allow multiple possible predictions, as there is usually not a single valid trajectory.</p> <p>Some approaches such as ChauffeurNet <a class="citation" href="#bansal2018chauffeutnet">(Bansal et al., 2018)</a> use a high-levelscene representation (road map, traffic lights, speed limit, route, dynamic bounding boxes, etc.). More recently, MultiPath <a class="citation" href="#chai2019multipath">(Chai et al., 2019)</a> uses trajectory anchors, used in one-step object detection, extracted from the training data for ego vehicle prediction. <a class="citation" href="#hong2019rules">(Hong et al., 2019)</a> use a high level representation which includes some dynamic context. In contrast, we choose to leverage also low level sensor data, here Lidar point clouds and camera image. In that domain, recent approaches address the variation in agent behaviors by predicting multiple trajectories, often in a stochastic way. Many works, e.g., PRECOG <a class="citation" href="#rhinehart2019precog">(Rhinehart et al., 2019)</a>, MFP <a class="citation" href="#tang2019mfp">(Tang &amp; Salakhutdinov, 2019)</a>, SocialGAN <a class="citation" href="#gupta2018socialgan">(Gupta et al., 2018)</a> and others <a class="citation" href="#rhinehart2018deepim">(Rhinehart et al., 2018)</a>, focus on this aspect through a probabilistic framework on the network output or latent representations, producing multiple trajectories for ego vehicle, nearby vehicles or both. <a class="citation" href="#phan2020covernet">(Phan-Minh et al., 2020)</a> generate a trajectory set, then classify correct trajectories. <a class="citation" href="#marchetti2020mantra">(Marchetti et al., 2020)</a> generate multiple futures from encodings of similar trajectories stored in a memory. <a class="citation" href="#ohn2020learning">(Ohn-Bar et al., 2020)</a> learn a weighted mixture of expert policies trained to mimic agents with specific behaviors. In PRECOG, <a class="citation" href="#rhinehart2019precog">(Rhinehart et al., 2019)</a> advance a probabilistic formulation that explicitly models interactions between agents, using latent variables to model their plausible reactions, with the possibility to precondition the trajectory of the ego vehicle by a goal.</p> <h2 id="plop-method">PLOP method</h2> <h3 id="contributions">Contributions</h3> <p>Our main goal is to produce a trajectory prediction which can be used to drive the ego vehicle relying on a conditional imitation learning algorithm, conditioned by a navigation command for the ego vehicle (e.g., “turn right”). To do so, we propose a single-shot, anchor-less trajectory prediction method, based on Mixture Desity Networks (MDNs) and polynomial trajectory constraints, relying only on on-board sensors which relaxes the HD map requirement and allow more flexibility for driving in the real world. The polynomial formulation ensures that the predicted trajectories are coherent and smooth, while providing more learning flexibility through the extra parameters. We find that this mitigates training instability and mode collapse that are common to MDNs <a class="citation" href="#cui2019multimodal">(Cui et al., 2019)</a>. PLOP is trainable end-to-end from imitation learning, where data is relatively easier to obtain and it is computationally efficient during both training and inference as it predicts trajectory coefficients in a single step, without requiring a RNN-based decoder. The polynomial function trajectory coefficients eschew the need for anchors <a class="citation" href="#chai2019multipath">(Chai et al., 2019)</a>, whose quality can vary across datasets.</p> <p>We propose an extensive evaluation of PLOP and show its effectiveness across datasets and settings. We conduct a comparison showing the improvement over state-of-the-art PRECOG <a class="citation" href="#rhinehart2019precog">(Rhinehart et al., 2019)</a> on the public dataset nuScenes <a class="citation" href="#holger2020nuscenes">(Caesar et al., 2020)</a>;</p> <p>Then for a better evaluation of the driving capacities of PLOP, we study closed loop performance for the ego vehicle, on simulation and with preliminary insights for real vehicle control.</p> <h3 id="network-architecture">Network architecture</h3> <p>PLOP takes as inputs: the ego and neighbor vehicles past positions represented as time sequences of x and y over the last 2 seconds, the frontal camera image of the ego vehicle, and 2 second history of bird’s eye views with a cell resolution of 1m square containing the lidar point cloud and the object detections information represented in Figure 2. The objects detections being the output of a state of the art perception algorithm.</p> <p><img src="/assets/img/posts/plop/inputs_plop_post.PNG" alt="plop_inputs" height="100%" width="100%"/></p> <div class="caption"><b>Figure 2. Image and Bird's eye view.</b> The left image is an example of a front camera input image of PLOP and the diagram on the right is a representation of the bird'eye view input.</div> <p>We pass these inputs through a multibranch neural network represented in Figure 3 to predict the ego vehicle future trajectory and two auxiliary tasks that are the future trajectory prediction for the neighbors vehicles and the semantic segmentation of the camera image.</p> <p><img src="/assets/img/posts/plop/archi_outputs.png" alt="plop_archi_outputs" height="100%" width="100%"/></p> <div class="caption"><b>Figure 3. PLOP's Architecture.</b> PLOP's architecture is reprented on the left while the polynomial multimodal gaussian trajectory representation is on the right</div> <p>The front camera image features, the bird’s eye view features and the ego vehicle past positions features are passed down to conditional fully connected architecture to output multiple future trajectories for the ego vehicle regarding the current navigation order. The trajectories are predicted using MDNs where gaussian means are generated using polynomial functions of degree 4 over x and y</p> <p>To improve the learning stability of our training and inject awareness about the scene layouts into the camera features we pass them through a U-Net decoder to output semantic segmentation and then use an auxiliary cross entropy loss. To improve the encoding of interactions between the differents agents of the scene in the bird’s eye features, we predict the future possible trajectories for each neighbor feeding the bird’s eye views encoding and its past positions encoded through a LSTM layer to a small fully connected network. The weights of LSTMs and fully connected layers are shared between all neighbors. This output allows us to get useful information about the ego vehicle environment that can be used online to improve the ego vehicle driving with safety collision checks for example.</p> <h2 id="offline-evaluation">Offline evaluation</h2> <p>To evaluate PLOP, we use the nuScenes dataset to train the trajectory loss along with the Audi <a class="citation" href="#geyer2019a2d2">(Geyer et al., 2019)</a> dataset to train the semantic segmentation loss. We choose to compare our method with the DESIRE <a class="citation" href="#lee2017desire">(Lee et al., 2017)</a> baseline and against two state of the art methods that are PRECOG and ESP <a class="citation" href="#rhinehart2019precog">(Rhinehart et al., 2019)</a> using the minimum Mean Squared Deviation metric to avoid penalizing valid trajectories that are not matching the ground truth. For one agent, meaning ego vehicle only, PRECOG and ESP have access to the future desired target position and PRECOG return significantly better results than PLOP but PLOP still reaches similar results as ESP. For multiple agents PLOP outperforms other presented methods . We note that the comparison if fairer for neighbor trajectories and the performance is relevant since they are by definition open loop.</p> <p><img src="/assets/img/posts/plop/offline_results.PNG" alt="plop_offline_results" height="60%" width="60%"/></p> <div class="caption"><b>Figure 4. Comparison with state-of-the-art:</b> Against the DESIRE, ESP and PRECOG for predicting a trajectory of 4 seconds into the future</div> <p>But we argue that such evaluation is not totally relevant for controling the ego vehicle in real conditions. Such metrics does not value the situations in which the errors are made, failing to brake at a traffic light is a critical error for example but it is quick and represent a very small part of the test set so it will impact very poorly the overall metrics. However, making a small constant error such as driving 2kph too slow over the whole test set set might be an acceptable and non impacting error but will lead to a considerable overall error. Also, using only offline metrics where the method can’t control the vehicle does not allow us to evaluate its capacities to react to its own mistakes.</p> <h2 id="online-evaluation-through-simulation">Online Evaluation through simulation</h2> <p>To simulate driving, we developped a data driven simulator that allows us to use real driving data to simulate applying the prediction to the ego vehicle. We can generate the input data that corresponds to the new vehicle position after following the trajectory using reprojections (for the image and the pointcloud), then use it to predict a new trajectory, and so on. This allows us the measure the performance in closed loop, and in particular to count failures which would have resulted in a takeover. We rely on 3 metrics: lateral (&gt;1m from expert), high speed (catching up to a vehicle 15% faster than the real vehicle up to 0.6s in the future) and low speed (&gt; 20kph under the expert speed) errors count.</p> <p><img src="/assets/img/posts/plop/simu_plop.png" alt="plop_online_results" height="100%" width="100%"/></p> <div class="caption"><b>Figure 5. Evaluation using the simulator.</b> Comparison with PLOP without semantic segmentation loss, Constant velocity baseline and Multi-Layer Perceptron baseline in the table on the left. Additionnal qualitative results about the errors positioning on the differents test tracks are on the right.</div> <p>We trained PLOP on an internal dataset combining both open road and urban test track and compared PLOP, PLOP without auxiliary semantic loss, the constant velocity baseline and a MLP baseline in our simulator using test data. We note that semantic segmentation improve the driving performance and that MLP has better offline metrics than constant velocity approach but still perform worse due to the simulated driving conditions. As expected, offline metrics are not discriminating enough for the online behavior since the best model checkpoints in simulation are not necessarily the ones with the better offline metrics. An additionnal ablation study where we remove mandatory information (such as the camera image input) shows that it may even be dangerous to trust them blindly.</p> <div class="publication-teaser"> <iframe width="560" height="315" src="https://www.youtube.com/embed/94FwahFmc5A?start=94" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> </div> <h2 id="conclusion">Conclusion</h2> <p>In this work, we demonstrate the interest of our multi-input multimodal approach PLOP for vehicle trajectory prediction in an urban environment. Our architecture leverages frontal camera and Lidar inputs, to produce multiple trajectories using reparameterized Mixture Density Networks, with an auxiliary semantic segmentation task. We show that we can improve open loop state-of-the-art performance in a multi-agent system, by evaluating the vehicle trajectories from the nuScenes dataset. We also provide a simulated closed loop evaluation, to go towards real vehicle online application. Please check out our paper along with supplementary materials for greater details about our approach and experiments and feel free to contact us for any question.</p> <h2 id="references">References</h2> <ol class="bibliography"><li> <div class="row"> Holger Caesar,&nbsp;Varun Bankiti,&nbsp;Alex H. Lang,&nbsp;Sourabh Vora,&nbsp;Venice Erin Liong,&nbsp;Qiang Xu,&nbsp;Anush Krishnan,&nbsp;Yu Pan,&nbsp;Giancarlo Baldan,&nbsp;and&nbsp;Oscar Beijbom.&nbsp; nuScenes: A Multimodal Dataset for Autonomous Driving.&nbsp; <em>In cvpr</em>,&nbsp;2020. </div> </li> <li> <div class="row"> Mayank Bansal,&nbsp;Alex Krizhevsky,&nbsp;and&nbsp;Abhijit S. Ogale.&nbsp; ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst.&nbsp; <em>CoRR</em>,&nbsp;2018. </div> </li> <li> <div class="row"> Yuning Chai,&nbsp;Benjamin Sapp,&nbsp;Mayank Bansal,&nbsp;and&nbsp;Dragomir Anguelov.&nbsp; MultiPath: Multiple Probabilistic Anchor Trajectory Hypotheses for Behavior Prediction.&nbsp; Oct&nbsp;2019. </div> </li> <li> <div class="row"> Joey Hong,&nbsp;Benjamin Sapp,&nbsp;and&nbsp;James Philbin.&nbsp; Rules of the Road: Predicting Driving Behavior with a Convolutional Model of Semantic Interactions.&nbsp; <em>CoRR</em>,&nbsp;2019. </div> </li> <li> <div class="row"> Nicholas Rhinehart,&nbsp;Rowan McAllister,&nbsp;Kris Kitani,&nbsp;and&nbsp;Sergey Levine.&nbsp; Precog: Prediction conditioned on goals in visual multi-agent settings.&nbsp; <em>In iccv</em>,&nbsp;2019. </div> </li> <li> <div class="row"> Charlie Tang,&nbsp;and&nbsp;Russ R Salakhutdinov.&nbsp; Multiple futures prediction.&nbsp; <em>In Advances in Neural Information Processing Systems</em>,&nbsp;2019. </div> </li> <li> <div class="row"> Agrim Gupta,&nbsp;Justin Johnson,&nbsp;Li Fei-Fei,&nbsp;Silvio Savarese,&nbsp;and&nbsp;Alexandre Alahi.&nbsp; Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks.&nbsp; <em>CoRR</em>,&nbsp;2018. </div> </li> <li> <div class="row"> Nicholas Rhinehart,&nbsp;Rowan McAllister,&nbsp;and&nbsp;Sergey Levine.&nbsp; Deep Imitative Models for Flexible Inference, Planning, and Control.&nbsp; <em>CoRR</em>,&nbsp;2018. </div> </li> <li> <div class="row"> Tung Phan-Minh,&nbsp;Elena Corina Grigore,&nbsp;Freddy A Boulton,&nbsp;Oscar Beijbom,&nbsp;and&nbsp;Eric M Wolff.&nbsp; Covernet: Multimodal behavior prediction using trajectory sets.&nbsp; <em>In cvpr</em>,&nbsp;2020. </div> </li> <li> <div class="row"> Francesco Marchetti,&nbsp;Federico Becattini,&nbsp;Lorenzo Seidenari,&nbsp;and&nbsp;Alberto Del Bimbo.&nbsp; Mantra: Memory augmented networks for multiple trajectory prediction.&nbsp; <em>In cvpr</em>,&nbsp;2020. </div> </li> <li> <div class="row"> Eshed Ohn-Bar,&nbsp;Aditya Prakash,&nbsp;Aseem Behl,&nbsp;Kashyap Chitta,&nbsp;and&nbsp;Andreas Geiger.&nbsp; Learning Situational Driving.&nbsp; <em>In cvpr</em>,&nbsp;2020. </div> </li> <li> <div class="row"> Henggang Cui,&nbsp;Vladan Radosavljevic,&nbsp;Fang-Chieh Chou,&nbsp;Tsung-Han Lin,&nbsp;Thi Nguyen,&nbsp;Tzu-Kuo Huang,&nbsp;Jeff Schneider,&nbsp;and&nbsp;Nemanja Djuric.&nbsp; Multimodal trajectory predictions for autonomous driving using deep convolutional networks.&nbsp; <em>In icra</em>,&nbsp;2019. </div> </li> <li> <div class="row"> Jakob Geyer,&nbsp;Yohannes Kassahun,&nbsp;Mentar Mahmudi,&nbsp;Xavier Ricou,&nbsp;Rupesh Durgesh,&nbsp;Andrew S. Chung,&nbsp;Lorenz Hauswald,&nbsp;Viet Hoang Pham,&nbsp;Maximilian Mühlegg,&nbsp;Sebastian Dorn,&nbsp;Tiffany Fernandez,&nbsp;Martin Jänicke,&nbsp;Sudesh Mirashi,&nbsp;Chiragkumar Savani,&nbsp;Martin Sturm,&nbsp;Oleksandr Vorobiov,&nbsp;and&nbsp;Peter Schuberth.&nbsp; A2D2: AEV Autonomous Driving Dataset.&nbsp; 2019. </div> </li> <li> <div class="row"> Namhoon Lee,&nbsp;Wongun Choi,&nbsp;Paul Vernaza,&nbsp;Chris Choy,&nbsp;Philip Torr,&nbsp;and&nbsp;Manmohan Chandraker.&nbsp; DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents.&nbsp; <em>In </em>,&nbsp;Jul&nbsp;2017. </div> </li></ol>]]></content><author><name></name></author><category term="driving"/><category term="multi-sensor"/><summary type="html"><![CDATA[Thibault Buhet, Emilie Wirbel, Andrei Bursuc and Xavier Perrotton]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io//assets/img/publications/plop/plop.png"/><media:content medium="image" url="https://valeoai.github.io//assets/img/publications/plop/plop.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">Is Deep Reinforcement Learning Really Superhuman on Atari?</title><link href="https://valeoai.github.io//posts/saber" rel="alternate" type="text/html" title="Is Deep Reinforcement Learning Really Superhuman on Atari?"/><published>2020-10-19T00:00:00+00:00</published><updated>2020-10-19T00:00:00+00:00</updated><id>https://valeoai.github.io//posts/saber</id><content type="html" xml:base="https://valeoai.github.io//posts/saber"><![CDATA[<p><em>This post describes our recent <a href="https://arxiv.org/pdf/1908.04683.pdf">work</a> on Deep Reinforcement Learning (DRL) on the Atari benchmark. DRL appears today as one of the closest paradigm to Artificial General Intelligence and large progress in this field has been enabled by the popular Atari benchmark. However, training and evaluation protocols on Atari vary across papers leading to biased comparisons, difficulty in reproducing results and in estimating the true contributions of the methods. Here we attempt to mitigate this problem with SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms. SABER allows us to compare multiple methods under the same conditions against a human baseline and to note that previous claims of superhuman performance on DRL do not hold. Finally, we propose a new state-of-the-art algorithm R-IQN combining Rainbow with Implicit Quantile Networks (IQN). Our code is <a href="https://github.com/valeoai/rainbow-iqn-apex">available</a>.</em></p> <p>Deep Reinforcement Learning is a learning scheme based on trial-and-error in which an agent learns an optimal policy from its own experiments and a reward signal. The goal of the agent is to maximize the sum of future accumulated rewards and thus the agent needs to think about sequences of actions rather than instantaneous ones. The Atari benchmark is valuable for evaluating general AI algorithms as it includes more than 50 games displaying high variability in the task to solve, ranging from simple paddle control in the ball game Pong to complex labyrinth exploration in Montezuma’s Revenge which remains unsolved by general algorithms up to today.</p> <p>We notice however that training and evaluation procedures on Atari can be different from paper to paper and thus leading to bias in comparison. Moreover this leads to difficulties to reproduce results of published works as some training or evaluation parameters are barely explained or sometimes not mentioned. In order to facilitate reproducible and comparable DRL, we introduce SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms. Furthermore, we introduce a human world record baseline and argue that previous claims of superhuman performance of DRL might not hold. Finally, we propose a new state-of-the-art algorithm R-IQN by combining the current state-of-the-art <strong>Rainbow</strong> <a class="citation" href="#hessel2017rainbow">(Hessel et al., 2018)</a> along with Implicit Quantile Networks (<strong>IQN</strong> <a class="citation" href="#dabney2018implicit">(Dabney et al., 2018)</a>). We release an open-source <a href="https://github.com/valeoai/rainbow-iqn-apex">implementation</a> of distributed R-IQN following the ideas from Ape-X!</p> <h2 id="dqns-human-baseline-vs-human-world-record-on-atari-games">DQN’s human baseline vs human world record on Atari Games</h2> <p>A common way to evaluate AI for games is to let agents compete against the best humans. Recent examples for DRL include the victory of <a href="https://deepmind.com/alphago-korea">AlphaGo versus Lee Sedol</a> for Go, <a href="https://openai.com/blog/openai-five/">OpenAI Five</a> on Dota 2 or <a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii">AlphaStar versus Mana</a> for StarCraft 2. For this reason one of the most used metrics for evaluating RL agents on Atari is to compare them to the human baseline introduced in DQN.</p> <p>Previous works use the normalized human score, <em>i.e</em>., 0% is the score of a random player and 100% is the score of the human baseline, which allows one to summarize the performance on the whole Atari set in one number, instead of individually comparing raw scores for each of the 61 games. However we argue that this human baseline is far from being representative of the best human player, which means that using it to claim superhuman performance is misleading.</p> <p>The current world records are available online for 58 of the 61 evaluated Atari games. For example, on VideoPinball, the world record is 50,000 times higher than the human baseline of DQN. Evaluating these world records scores using the usual human normalized score has a median of 4,400% and a mean of 99,300% (see Figure below for details on each game), to be compared to 200% and 800% of the current state-of-the-art Rainbow!</p> <p><img src="/assets/img/posts/saber/pro_vs_beginner.png" alt="pro_vs_beginner" height="60%" width="60%"/></p> <div class="caption"><b>Figure 1: World record scores vs. the usual beginner human baseline</b> (log scale) <a class="citation" href="#dabney2018implicit">(Dabney et al., 2018)</a> baseline can be up to 50k times lower than registered world records. Beating that baseline does not necessarily make the agent superhuman.</div> <p>We estimate that evaluating the algorithms under the world record baseline instead of the DQN human baseline will give a better view of the gap remaining between best human players and DRL agents. Results are confirming this, as Rainbow reaches only a median human-normalized score of 3% (see Figure 2 below) meaning that for half of Atari games, the agent doesn’t even reach 3% of the way from random to best human run.</p> <p>In the video below we analyze agents previously claimed as above human-level but far from the world record. By taking a closer look at the AI playing, we discovered that on most of Atari games DRL agents fail to understand the goal of the game. Sometimes they just don’t explore other levels than the initial one, sometimes they are stuck in a loop giving small amount of reward, etc. A compelling example of this is the game Riverraid. In this game, the agent must shoot everything and take fuel to survive: the agent dies if there is a collision with an enemy or if out of fuel. But as shooting fuel actually gives points, the agent doesn’t understand that he could play way longer and win even more points by actually taking this fuel bonus and not shooting them!</p> <p align="center"> <iframe width="560" height="315" src="https://www.youtube.com/embed/oH6P3ksYLek" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" align="center"></iframe> </p> <h2 id="saber-a-standardized-atari-benchmark-for-general-reinforcement-learning-algorithms">SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms</h2> <p>In our work we extend the recommendations proposed by <a class="citation" href="#machado2018revisiting">(Machado et al., 2018)</a> They also point out divergences in training and evaluating agents on Atari. Consequently they compile and propose a set of recommendations for more reproducible and comparable RL including sticky actions, ignoring life signal and using full action set.</p> <p>There is however one major parameter that is left out in <a class="citation" href="#machado2018revisiting">(Machado et al., 2018)</a>: the maximum number of frames allowed per episode. This parameter ends the episode after a fixed number of time steps even if the game is not over. In most of the recent works, this is set to 30 min of game play (i.e., 108k frames) and only to 5 min in some others (i.e.,18k frames). This means that the reported scores can not be compared fairly. For example, in easy games (e.g., Atlantis), the agent never dies and the score is more or less linear with the allowed time: the reported score will be 6 times higher if capped at 30 minutes instead of 5 minutes.</p> <p>Another issue with this time cap comes from the fact that some games are designed for much longer gameplay than 5 or 30 minutes. On those games (e.g., Atlantis, Video Pinball, Enduro) the scores reported of Ape-X, Rainbow and IQN are almost the same. This is due to all agents reaching the time limit and getting the maximum possible score in 30 minutes: the difference in scores is due to minor variations, not algorithmic difference and thus the comparison is not significant. As a consequence, the more successful agents are, the more games are incomparable because they reach the maximum possible score in the time cap while still being far behind human world record.</p> <p>This parameter can also be a source of ambiguity and error. The best score on Atlantis (2,311,815) is reported by Proximal Policy Optimization by <a class="citation" href="#schulman2017proximal">(Schulman et al., 2017)</a>, however this score is likely wrong: it seems impossible to reach in 30 minutes only! The first distributional paper by (<a href="http://proceedings.mlr.press/v70/bellemare17a.html">Bellemare et al.</a>) also did this mistake and reported wrong results before adding an erratum in a later version on <em>ArXiv</em> <a class="citation" href="#bellemare2017distributional">(Bellemare et al., 2017)</a>.</p> <p><img src="/assets/img/posts/saber/saber_params.png" alt="saber_params" height="60%" width="60%"/></p> <div class="caption"><b>Table 1:</b> Game parameters of SABER.</div> <p>We first re-implemented the current state-of-the-art Rainbow and evaluated it on SABER. We noticed that the same algorithm under different evaluation settings can lead to significantly different results. This showed again the necessity of a common and standardized benchmark, more details can be found in the paper.</p> <p>Then we implemented a new algorithm, R-IQN, by replacing the C51 algorithm (which is one of the 6 components of Rainbow) by Implicit Quantile Network (IQN). Both C51 and IQN belong to the field of Distributional RL which aims to predict the full distribution of the Q-value function instead of just predicting the mean of it. The fundamental difference between these two algorithms is how they parametrize the Q-value distribution. C51, which is the first algorithm of Distributional RL, approximates the Q-value as a categorical distribution with fixed support and just learns the mass to attribute to each category. On the other hand, IQN approximates the Q-value with quantile regression and both the support and the mass arelearned resulting in a major improvement in performance and data-efficiency over C51. As IQN arises much better performance than C51 while still designed for the same goal (predict the full distribution of the Q-function), combining Rainbow with IQN is relevant and natural.</p> <p>As shown in the graph below, R-IQN outperforms Rainbow and thus becomes the new state-of-the-art on Atari. However, we acknowledge that in order to make a more confident state-of-the-art claim we should run multiple times with different seeds. Testing an increased number of random seeds across the 60 Atari games is a computationally costly endeavor and is beyond the scope of this study. We test the stability of the performances of R-IQN across 5 random seeds on a subset of 14 games. We compare against Rainbow and report similar results in Figure 3.</p> <p><img src="/assets/img/posts/saber/median_human_normalised_Rainbow_vs_Rainbow_IQN.png" alt="median_rainbow_vs_us" height="90%" width="90%"/></p> <div class="caption"><b>Figure 2: Comparison of Rainbow and Rainbow-IQN on SABER.</b> We report median normalized scores w.r.t training steps.</div> <p><img src="/assets/img/posts/saber/median_human_normalised_5_seeds_both_rainbow_riqn.png" alt="median_rainbow_vs_us_seeds" height="90%" width="90%"/></p> <div class="caption"><b>Figure 3: Comparison of Rainbow and Rainbow-IQN on a subset of 14 games using 5 seeds.</b> We report median normalized scores w.r.t training steps.</div> <h2 id="open-source-implementation-of-distributed-rainbow-iqn-r-iqn-ape-x">Open-source implementation of distributed Rainbow-IQN: R-IQN Ape-X</h2> <p>We release our code of a distributed version of Rainbow-IQN following ideas from Ape-X <a class="citation" href="#horgan2018distributed">(Horgan et al., 2018)</a>. The distributed part is our main practical advantage over some existing DRL repositories (particularly <a href="https://github.com/google/dopamine">Dopamine</a> a popular open-source implementation of DQN, C51, IQN and a <em>small-Rainbow</em> but in which all algorithms are single worker). Indeed, when using DRL algorithms for other tasks (other than Atari and MuJoCO) a major bottleneck is the <em>speed</em> of the environment. DRL algorithms often need a huge amount of data before reaching reasonable performance. This amount may be practically impossible to reach if the environment is real-time and if collecting data from multiple environments at the same time is not possible.</p> <p>Building upon ideas from Ape-X, we use REDIS as a side server to store our replay memory. Multiple actors will act in their own instances of the environment to fill as fast as they can the replay memory. Finally, the learner will sample from this replay memory (the learner is actually completely independent of the environment) for backprop. The learner will also periodically update the weight of each actor as shown in the schema below.</p> <p><img src="/assets/img/posts/saber/appex_arch.png" alt="apex_arch" height="60%" width="60%"/></p> <div class="caption"><b>Figure 4: Ape-X architecture.</b> image taken from <a class="citation" href="#horgan2018distributed">(Horgan et al., 2018)</a></div> <p>This scheme allowed us to use R-IQN Ape-X for the task of autonomous driving using <a href="http://carla.org/">CARLA</a> as environment. This enabled us to win the <a href="https://carlachallenge.org/results-challenge-2019/">CARLA Challenge</a> on Track 2 <em>Cameras Only</em> showing the strength of R-IQN Ape-X as a general algorithm.</p> <h2 id="conclusion">Conclusion</h2> <p>In this work, we confirm the impact of standardized guidelines for DRL evaluation, and build a consolidated benchmark, SABER. In order to provide a more significant comparison, we build a new baseline based on human world records and show that the state-of-the-art Rainbow agent is in fact far from human world record performance. In the paper we share possible reasons for this failure. We hope that SABER will facilitate better comparisons and enable new exciting methods to prove their effectiveness without ambiguity.</p> <p>Check out our paper to find out more about intuitions, experiments and interpretations.</p> <h2 id="references">References</h2> <ol class="bibliography"><li> <div class="row"> Matteo Hessel,&nbsp;Joseph Modayil,&nbsp;Hado Van Hasselt,&nbsp;Tom Schaul,&nbsp;Georg Ostrovski,&nbsp;Will Dabney,&nbsp;Dan Horgan,&nbsp;Bilal Piot,&nbsp;Mohammad Azar,&nbsp;and&nbsp;David Silver.&nbsp; Rainbow: Combining improvements in deep reinforcement learning.&nbsp; <em>In AAAI</em>,&nbsp;2018. </div> </li> <li> <div class="row"> Will Dabney,&nbsp;Georg Ostrovski,&nbsp;David Silver,&nbsp;and&nbsp;Rémi Munos.&nbsp; Implicit quantile networks for distributional reinforcement learning.&nbsp; <em>In ICML</em>,&nbsp;2018. </div> </li> <li> <div class="row"> Marlos C Machado,&nbsp;Marc G Bellemare,&nbsp;Erik Talvitie,&nbsp;Joel Veness,&nbsp;Matthew Hausknecht,&nbsp;and&nbsp;Michael Bowling.&nbsp; Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents.&nbsp; <em>Journal of Artificial Intelligence Research</em>,&nbsp;2018. </div> </li> <li> <div class="row"> John Schulman,&nbsp;Filip Wolski,&nbsp;Prafulla Dhariwal,&nbsp;Alec Radford,&nbsp;and&nbsp;Oleg Klimov.&nbsp; Proximal policy optimization algorithms.&nbsp; <em>arXiv preprint arXiv:1707.06347</em>,&nbsp;2017. </div> </li> <li> <div class="row"> Marc G Bellemare,&nbsp;Will Dabney,&nbsp;and&nbsp;Rémi Munos.&nbsp; A distributional perspective on reinforcement learning.&nbsp; <em>arXiv preprint arXiv:1707.06887</em>,&nbsp;2017. </div> </li> <li> <div class="row"> Dan Horgan,&nbsp;John Quan,&nbsp;David Budden,&nbsp;Gabriel Barth-Maron,&nbsp;Matteo Hessel,&nbsp;Hado Van Hasselt,&nbsp;and&nbsp;David Silver.&nbsp; Distributed prioritized experience replay.&nbsp; <em>arXiv preprint arXiv:1803.00933</em>,&nbsp;2018. </div> </li></ol>]]></content><author><name></name></author><category term="deep reinforcement learning"/><summary type="html"><![CDATA[Marin Toromanoff, Emilie Wirbel]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io//assets/img/posts/saber/space_invaders.jpg"/><media:content medium="image" url="https://valeoai.github.io//assets/img/posts/saber/space_invaders.jpg" xmlns:media="http://search.yahoo.com/mrss/"/></entry></feed>