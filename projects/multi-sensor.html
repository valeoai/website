<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Multi-sensor perception | valeo.ai - valeo.ai research page</title> <meta name="author" content=" "/> <meta name="description" content="Automated driving relies first on a diverse range of sensors, like fish-eye and pinhole camera rigs, LiDARs, microphones, radars, and ultrasonics. Exploiting at best the outputs of each of these sensors at any instant is fundamental to understand the complex environment of the vehicle and gain robustness"/> <meta name="keywords" content="computer vision, ai, valeo, artificial intelligence, research, deep learning"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/valeoai_logo_256x256.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://valeoai.github.io//projects/multi-sensor"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"> <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script> <script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"[%",right:"%]",display:!0},{left:"$",right:"$",display:!1}]})});</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><img src="/assets/img/valeoai_logo.png" alt="valeo.ai" class="title-logo" height="24px"></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"></a> </li> <li class="nav-item "> <a class="nav-link" href="/team/">Team</a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Code &amp; Data</a> </li> <li class="nav-item "> <a class="nav-link" href="/posts/">Posts</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <h1>Multi-sensor perception</h1> <p>Automated driving relies first on a diverse range of sensors, like fish-eye and pinhole camera rigs, LiDARs, microphones, radars, and ultrasonics. Exploiting at best the outputs of each of these sensors at any instant is fundamental to understand the complex environment of the vehicle and gain robustness</p> <h2>Selected publications</h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4"><img class="preview rounded" src="/assets/img/publications/2024_occfeat/occfeat_teaser.png"></div> <div id="OccFeat: Self-supervised Occupancy Feature Prediction for Pretraining BEV Segmentation Networks" class="col-sm-8"> <div class="title"> <a href="/publications/occfeat/" target="_blank"> OccFeat: Self-supervised Occupancy Feature Prediction for Pretraining BEV Segmentation Networks </a> </div> <div class="author"> Sophia Sirko-Galouchenko, Alexandre Boulch, Spyros Gidaris, Andrei Bursuc, Antonin Vobecky, Patrick Pérez, Renaud Marlet </div> <div class="periodical"> <em>CVPR Workshop on Autonomous Driving (WAD), 2024</em> </div> <div class="links"> <a href="https://arxiv.org/abs/2404.14027" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> <a href="https://github.com/valeoai/Occfeat" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> </div> </div> </li> <hr> <li> <div class="row"> <div class="col-sm-4"><img class="preview rounded" src="/assets/img/publications/2024_pointbev/pointbev.PNG"></div> <div id="PointBeV: A Sparse Approach to BeV Predictions" class="col-sm-8"> <div class="title"> <a href="/publications/pointbev/" target="_blank"> PointBeV: A Sparse Approach to BeV Predictions </a> </div> <div class="author"> Loïck Chambon, Éloi Zablocki, Mickaël Chen, Florent Bartoccioni, Patrick Pérez, Matthieu Cord </div> <div class="periodical"> <em>Computer Vision and Pattern Recognition, 2024</em> </div> <div class="links"> <a href="https://arxiv.org/abs/2312.00703" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> <a href="https://github.com/valeoai/pointbev" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> </div> </div> </li> <hr> <li> <div class="row"> <div class="col-sm-4"><img class="preview rounded" src="/assets/img/publications/2024_valeo4cast/valeo4cast.PNG"></div> <div id="Valeo4Cast: A Modular Approach to End-to-End Forecasting" class="col-sm-8"> <div class="title"> <a href="/publications/valeo4cast/" target="_blank"> Valeo4Cast: A Modular Approach to End-to-End Forecasting </a> </div> <div class="author"> Yihong Xu, Éloi Zablocki, Alexandre Boulch, Gilles Puy, Mickael Chen, Florent Bartoccioni, Nermin Samet, Oriane Siméoni, Spyros Gidaris, Tuan-Hung Vu, Andrei Bursuc, Eduardo Valle, Renaud Marlet, Matthieu Cord </div> <div class="periodical"> <em>Winning solution to the "Unified Detection, Tracking and Forecasting" Argoverse 2 challenge @CVPR Worshop on Autonomous Driving (WAD), 2024</em> </div> <div class="links"> <a href="https://arxiv.org/abs/2406.08113" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><img class="preview rounded" src="/assets/img/publications/2023_pop3d/pop3d_teaser.png"></div> <div id="POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images" class="col-sm-8"> <div class="title"> <a href="/publications/pop3d/" target="_blank"> POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images </a> </div> <div class="author"> Antonin Vobecky, Oriane Siméoni, David Hurych, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Josef Sivic </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS), 2023</em> </div> <div class="links"> <a href="https://arxiv.org/abs/2401.09413" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> <a href="https://github.com/vobecant/POP3D" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> </div> </div> </li> <hr> <li> <div class="row"> <div class="col-sm-4"><img class="preview rounded" src="/assets/img/publications/2024_rmcl/rMCL.png"></div> <div id="Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis" class="col-sm-8"> <div class="title"> <a href="/publications/rmcl/" target="_blank"> Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis </a> </div> <div class="author"> Victor Letzelter, Mathieu Fontaine, Mickaël Chen, Patrick Pérez, Slim Essid, and Gaël Richard </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS), 2023</em> </div> <div class="links"> <a href="https://arxiv.org/abs/2311.01052" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> <a href="https://github.com/Victorletzelter/code-rMCL" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://recorder-v3.slideslive.com/?share=86327&amp;s=9b08dcaf-9905-4f10-a399-f6aa4aee047f" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><img class="preview rounded" src="/assets/img/publications/2022_lara/lara.png"></div> <div id="LaRa: Latents and Rays for Multi-Camera Bird's-Eye-View Semantic Segmentation" class="col-sm-8"> <div class="title"> <a href="/publications/lara/" target="_blank"> LaRa: Latents and Rays for Multi-Camera Bird's-Eye-View Semantic Segmentation </a> </div> <div class="author"> Florent Bartoccioni, Éloi Zablocki, Andrei Bursuc, Patrick Pérez, Matthieu Cord, Karteek Alahari </div> <div class="periodical"> <em>Conference on Robot Learning, 2022</em> </div> <div class="links"> <a href="https://arxiv.org/abs/2206.13294" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> <a href="https://github.com/valeoai/LaRa" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> </div> </div> </li> <hr> <li> <div class="row"> <div class="col-sm-4"><img class="preview rounded" src="/assets/img/publications/2022_lidartouch/lidartouch.png"></div> <div id="LiDARTouch: Monocular metric depth estimation with a few-beam LiDAR" class="col-sm-8"> <div class="title"> <a href="/publications/lidartouch/" target="_blank"> LiDARTouch: Monocular metric depth estimation with a few-beam LiDAR </a> </div> <div class="author"> Florent Bartoccioni, Éloi Zablocki, Patrick Pérez, Matthieu Cord, Karteek Alahari </div> <div class="periodical"> <em>Computer Vision and Image Understanding (CVIU), 2022</em> </div> <div class="links"> <a href="https://arxiv.org/abs/2109.03569" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> <a href="https://github.com/F-Barto/LiDARTouch" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> </div> </div> </li> <hr> <li> <div class="row"> <div class="col-sm-4"><img class="preview rounded" src="/assets/img/publications/2022_multimodal_3d_gan/2022_multimodal_3d_gan.PNG"></div> <div id="Multi-Modal 3D GAN for Urban Scenes" class="col-sm-8"> <div class="title"> <a href="/publications/multimodal_3d_gan/" target="_blank"> Multi-Modal 3D GAN for Urban Scenes </a> </div> <div class="author"> Loïck Chambon, Mickaël Chen, Tuan-Hung Vu, Alexandre Boulch, Andrei Bursuc, Matthieu Cord, Patrick Pérez </div> <div class="periodical"> <em>NeurIPS Machine Learning for Autonomous Driving Workshop, 2022</em> </div> <div class="links"> <a href="https://ml4ad.github.io/files/papers2022/Multi-Modal%203D%20GAN%20for%20Urban%20Scenes.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> <a href="https://neurips.cc/virtual/2022/59785" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a> </div> </div> </div> </li> <hr> <li> <div class="row"> <div class="col-sm-4"><img class="preview rounded" src="/assets/img/publications/2022_ds/ds_overview.png"></div> <div id="Drive&amp;Segment: Unsupervised Semantic Segmentation of Urban Scenes via Cross-modal Distillation" class="col-sm-8"> <div class="title"> <a href="/publications/drive-segment/" target="_blank"> Drive&amp;Segment: Unsupervised Semantic Segmentation of Urban Scenes via Cross-modal Distillation </a> </div> <div class="author"> Antonin Vobecky, Oriane Siméoni, David Hurych, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Josef Sivic </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV), 2022</em><span class="award">oral</span> </div> <div class="links"> <a href="https://arxiv.org/abs/2203.11160" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> <a href="https://github.com/vobecant/DriveAndSegment" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> </div> </div> </li> <hr> <li> <div class="row"> <div class="col-sm-4"><img class="preview rounded" src="/assets/img/publications/2022_radial/radial_teaser.png"></div> <div id="Raw High-Definition Radar for Multi-Task Learning" class="col-sm-8"> <div class="title"> <a href="/publications/radial/" target="_blank"> Raw High-Definition Radar for Multi-Task Learning </a> </div> <div class="author"> Julien Rebut, Arthur Ouaknine, Waqas Malik, and Patrick Pérez </div> <div class="periodical"> <em>Computer Vision and Pattern Recognition, 2022</em> </div> <div class="links"> <a href="https://arxiv.org/abs/2112.10646" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> <a href="https://github.com/valeoai/RADIal" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> </div> </div> </li> <hr> <li> <div class="row"> <div class="col-sm-4"><img class="preview rounded" src="/assets/img/publications/2022_xmossda/xmossda.png"></div> <div id="Cross-modal Learning for Domain Adaptation in 3D Semantic Segmentation" class="col-sm-8"> <div class="title"> <a href="/publications/xmossda/" target="_blank"> Cross-modal Learning for Domain Adaptation in 3D Semantic Segmentation </a> </div> <div class="author"> Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Émilie Wirbel, and Patrick Pérez </div> <div class="periodical"> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022</em> </div> <div class="links"> <a href="https://ieeexplore.ieee.org/document/9737217" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> <a href="https://github.com/valeoai/xmuda_journal" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><img class="preview rounded" src="/assets/img/publications/mvrss/teaser.png"></div> <div id="Multi-View Radar Semantic Segmentation" class="col-sm-8"> <div class="title"> <a href="https://arthurouaknine.github.io/codeanddata/mvrss" target="_blank" rel="noopener noreferrer"> Multi-View Radar Semantic Segmentation </a> </div> <div class="author"> Arthur Ouaknine, Alasdair Newson, Patrick Pérez, Florence Tupin and Julien Rebut </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV), 2021</em> </div> <div class="links"> <a href="https://arthurouaknine.github.io/codeanddata/mvrss" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project page</a> <a href="https://arxiv.org/abs/2103.16214" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> <a href="https://github.com/valeoai/MVRSS" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><img class="preview rounded" src="/assets/img/publications/carrada/teaser.png"></div> <div id="CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations" class="col-sm-8"> <div class="title"> <a href="https://arthurouaknine.github.io/codeanddata/carrada" target="_blank" rel="noopener noreferrer"> CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations </a> </div> <div class="author"> Arthur Ouaknine, Alasdair Newson, Julien Rebut, Florence Tupin and Patrick Pérez </div> <div class="periodical"> <em>International Conference on Pattern Recognition (ICPR), 2020</em> </div> <div class="links"> <a href="https://arthurouaknine.github.io/codeanddata/carrada" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project page</a> <a href="https://arxiv.org/abs/2005.01456" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> <a href="https://github.com/valeoai/carrada_dataset" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> </div> </div> </li> <hr> <li> <div class="row"> <div class="col-sm-4"><img class="preview rounded" src="/assets/img/publications/plop/plop.png"></div> <div id="PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving" class="col-sm-8"> <div class="title"> <a href="/publications/plop/" target="_blank"> PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving </a> </div> <div class="author"> Thibault Buhet, Emilie Wirbel, Andrei Bursuc and Xavier Perrotton </div> <div class="periodical"> <em>Conference on Robot Learning (CoRL), 2020</em> </div> <div class="links"> <a href="https://arxiv.org/pdf/2003.08744.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> </div> </div> </div> </li> <hr> <li> <div class="row"> <div class="col-sm-4"><img class="preview rounded" src="/assets/img/publications/dynamic-mtl/qualitative.jpg"></div> <div id="Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems" class="col-sm-8"> <div class="title"> <a href="/publications/dynamic-mtl/" target="_blank"> Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems </a> </div> <div class="author"> Isabelle Leang, Ganesh Sistu, Fabian Burger, Andrei Bursuc, and Senthil Yogamani </div> <div class="periodical"> <em>IEEE International Conference on Intelligent Transportation Systems (ITSC), 2020</em> </div> <div class="links"> <a href="https://arxiv.org/abs/2001.02223" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> </div> </div> </div> </li> <hr> <li> <div class="row"> <div class="col-sm-4"><img class="preview rounded" src="/assets/img/publications/xmuda/xmuda.png"></div> <div id="xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation" class="col-sm-8"> <div class="title"> <a href="/publications/xmuda/" target="_blank"> xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation </a> </div> <div class="author"> Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Émilie Wirbel, and Patrick Pérez </div> <div class="periodical"> <em>Computer Vision and Pattern Recognition, 2020</em> </div> <div class="links"> <a href="https://arxiv.org/abs/1911.12676" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> <a href="https://github.com/valeoai/xmuda" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> </div> </div> </li> <hr> <li> <div class="row"> <div class="col-sm-4"><img class="preview rounded" src="/assets/img/publications/vrunet/crossing_norm_seq2.png"></div> <div id="VRUNet: Multi-Task Learning Model for Intent Prediction of Vulnerable Road Users" class="col-sm-8"> <div class="title"> <a href="/publications/vrunet/" target="_blank"> VRUNet: Multi-Task Learning Model for Intent Prediction of Vulnerable Road Users </a> </div> <div class="author"> Adithya Ranga, Filippo Giruzzi, Jagdish Bhanushali, Emilie Wirbel, Patrick Pérez, Tuan-Hung Vu, Xavier Perotton </div> <div class="periodical"> <em>Electronic Imaging, 2020</em> </div> <div class="links"> <a href="https://www.ingentaconnect.com/content/ist/ei/2020/00002020/00000016/art00012" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> </div> </div> </div> </li> </ol> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container">valeo.ai research page </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>